{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNet_3D_Cy_(2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk-_EXONRs9K",
        "colab_type": "text"
      },
      "source": [
        "3D U-net Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9JSnssMRri3",
        "colab_type": "code",
        "outputId": "854812c4-ef11-4a77-b833-9cd424605e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import os\n",
        "from tensorflow.keras.utils import get_file\n",
        "import tensorflow as tf # version 2.2.0-rc2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import Adam, SGD\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "!pip install pyyaml h5py  # Required to save models in HDF5 format\n",
        "\n",
        "cwd = os.getcwd()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzcB3oocQsmD",
        "colab_type": "text"
      },
      "source": [
        "Load MRI Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuiuoHPoNGRI",
        "colab_type": "code",
        "outputId": "edcdbaf5-ca31-4d74-81af-112c4cce7111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Create folders and define paths\n",
        "\n",
        "!mkdir data\n",
        "!mkdir results\n",
        "!mkdir saved_model\n",
        "\n",
        "DATA_PATH = '/content/datasets-promise12'\n",
        "RESULT_PATH = '/content/results'\n",
        "\n",
        "print('Image and label data downloaded: ')\n",
        "print('Result directory created: <%s>.' % os.path.abspath(RESULT_PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image and label data downloaded: \n",
            "Result directory created: </content/results>.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px7xkGmeOOzO",
        "colab_type": "code",
        "outputId": "f98f179e-ef6c-4648-cec8-a3559cc3e931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import zipfile\n",
        "!wget https://github.com/gu98/MPHY0041_Segmentation/blob/master/data/datasets-promise12.zip?raw=true\n",
        "!unzip datasets-promise12.zip?raw=true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-03 14:59:02--  https://github.com/gu98/MPHY0041_Segmentation/blob/master/data/datasets-promise12.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/gu98/MPHY0041_Segmentation/raw/master/data/datasets-promise12.zip [following]\n",
            "--2020-05-03 14:59:02--  https://github.com/gu98/MPHY0041_Segmentation/raw/master/data/datasets-promise12.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/gu98/MPHY0041_Segmentation/master/data/datasets-promise12.zip [following]\n",
            "--2020-05-03 14:59:02--  https://raw.githubusercontent.com/gu98/MPHY0041_Segmentation/master/data/datasets-promise12.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58033622 (55M) [application/zip]\n",
            "Saving to: ‘datasets-promise12.zip?raw=true’\n",
            "\n",
            "datasets-promise12. 100%[===================>]  55.34M   233MB/s    in 0.2s    \n",
            "\n",
            "2020-05-03 14:59:03 (233 MB/s) - ‘datasets-promise12.zip?raw=true’ saved [58033622/58033622]\n",
            "\n",
            "Archive:  datasets-promise12.zip?raw=true\n",
            "   creating: datasets-promise12/\n",
            "  inflating: datasets-promise12/label_train42.npy  \n",
            "  inflating: datasets-promise12/image_train27.npy  \n",
            "  inflating: datasets-promise12/image_train33.npy  \n",
            "  inflating: datasets-promise12/image_test10.npy  \n",
            "  inflating: datasets-promise12/image_test04.npy  \n",
            "  inflating: datasets-promise12/image_test05.npy  \n",
            "  inflating: datasets-promise12/image_test11.npy  \n",
            "  inflating: datasets-promise12/image_train32.npy  \n",
            "  inflating: datasets-promise12/image_train26.npy  \n",
            "  inflating: datasets-promise12/label_train43.npy  \n",
            "  inflating: datasets-promise12/label_train41.npy  \n",
            "  inflating: datasets-promise12/image_train18.npy  \n",
            "  inflating: datasets-promise12/image_train30.npy  \n",
            "  inflating: datasets-promise12/image_train24.npy  \n",
            "  inflating: datasets-promise12/image_test07.npy  \n",
            "  inflating: datasets-promise12/image_test13.npy  \n",
            "  inflating: datasets-promise12/image_test12.npy  \n",
            "  inflating: datasets-promise12/image_test06.npy  \n",
            "  inflating: datasets-promise12/image_train25.npy  \n",
            "  inflating: datasets-promise12/image_train31.npy  \n",
            "  inflating: datasets-promise12/image_train19.npy  \n",
            "  inflating: datasets-promise12/label_train40.npy  \n",
            "  inflating: datasets-promise12/label_train44.npy  \n",
            "  inflating: datasets-promise12/image_train35.npy  \n",
            "  inflating: datasets-promise12/image_train21.npy  \n",
            "  inflating: datasets-promise12/image_train09.npy  \n",
            "  inflating: datasets-promise12/image_test02.npy  \n",
            "  inflating: datasets-promise12/image_test16.npy  \n",
            "  inflating: datasets-promise12/image_test17.npy  \n",
            "  inflating: datasets-promise12/image_test03.npy  \n",
            "  inflating: datasets-promise12/image_train08.npy  \n",
            "  inflating: datasets-promise12/image_train20.npy  \n",
            "  inflating: datasets-promise12/image_train34.npy  \n",
            "  inflating: datasets-promise12/label_train45.npy  \n",
            "  inflating: datasets-promise12/label_train47.npy  \n",
            "  inflating: datasets-promise12/image_train22.npy  \n",
            "  inflating: datasets-promise12/image_train36.npy  \n",
            "  inflating: datasets-promise12/image_test29.npy  \n",
            "  inflating: datasets-promise12/image_test15.npy  \n",
            "  inflating: datasets-promise12/image_test01.npy  \n",
            "  inflating: datasets-promise12/image_test00.npy  \n",
            "  inflating: datasets-promise12/image_test14.npy  \n",
            "  inflating: datasets-promise12/image_test28.npy  \n",
            "  inflating: datasets-promise12/image_train37.npy  \n",
            "  inflating: datasets-promise12/image_train23.npy  \n",
            "  inflating: datasets-promise12/label_train46.npy  \n",
            "  inflating: datasets-promise12/label_train35.npy  \n",
            "  inflating: datasets-promise12/label_train21.npy  \n",
            "  inflating: datasets-promise12/label_train09.npy  \n",
            "  inflating: datasets-promise12/image_train44.npy  \n",
            "  inflating: datasets-promise12/image_train45.npy  \n",
            "  inflating: datasets-promise12/label_train08.npy  \n",
            "  inflating: datasets-promise12/label_train20.npy  \n",
            "  inflating: datasets-promise12/label_train34.npy  \n",
            "  inflating: datasets-promise12/label_train22.npy  \n",
            "  inflating: datasets-promise12/label_train36.npy  \n",
            "  inflating: datasets-promise12/image_train47.npy  \n",
            "  inflating: datasets-promise12/image_train46.npy  \n",
            "  inflating: datasets-promise12/label_train37.npy  \n",
            "  inflating: datasets-promise12/label_train23.npy  \n",
            "  inflating: datasets-promise12/label_train27.npy  \n",
            "  inflating: datasets-promise12/label_train33.npy  \n",
            "  inflating: datasets-promise12/image_train42.npy  \n",
            "  inflating: datasets-promise12/image_train43.npy  \n",
            "  inflating: datasets-promise12/label_train32.npy  \n",
            "  inflating: datasets-promise12/label_train26.npy  \n",
            "  inflating: datasets-promise12/label_train18.npy  \n",
            "  inflating: datasets-promise12/label_train30.npy  \n",
            "  inflating: datasets-promise12/label_train24.npy  \n",
            "  inflating: datasets-promise12/image_train41.npy  \n",
            "  inflating: datasets-promise12/image_train40.npy  \n",
            "  inflating: datasets-promise12/label_train25.npy  \n",
            "  inflating: datasets-promise12/label_train31.npy  \n",
            "  inflating: datasets-promise12/label_train19.npy  \n",
            "  inflating: datasets-promise12/label_train14.npy  \n",
            "  inflating: datasets-promise12/label_train00.npy  \n",
            "  inflating: datasets-promise12/label_train28.npy  \n",
            "  inflating: datasets-promise12/README.md  \n",
            "  inflating: datasets-promise12/label_train29.npy  \n",
            "  inflating: datasets-promise12/label_train01.npy  \n",
            "  inflating: datasets-promise12/label_train15.npy  \n",
            "  inflating: datasets-promise12/label_train03.npy  \n",
            "  inflating: datasets-promise12/label_train17.npy  \n",
            "  inflating: datasets-promise12/label_train16.npy  \n",
            "  inflating: datasets-promise12/label_train02.npy  \n",
            "  inflating: datasets-promise12/label_train06.npy  \n",
            "  inflating: datasets-promise12/label_train12.npy  \n",
            "  inflating: datasets-promise12/label_train13.npy  \n",
            "  inflating: datasets-promise12/label_train07.npy  \n",
            "  inflating: datasets-promise12/label_train39.npy  \n",
            "  inflating: datasets-promise12/label_train11.npy  \n",
            "  inflating: datasets-promise12/label_train05.npy  \n",
            "  inflating: datasets-promise12/image_train48.npy  \n",
            "  inflating: datasets-promise12/image_train49.npy  \n",
            "  inflating: datasets-promise12/label_train04.npy  \n",
            "  inflating: datasets-promise12/label_train10.npy  \n",
            "  inflating: datasets-promise12/label_train38.npy  \n",
            "  inflating: datasets-promise12/image_train06.npy  \n",
            "  inflating: datasets-promise12/image_train12.npy  \n",
            "  inflating: datasets-promise12/image_test25.npy  \n",
            "  inflating: datasets-promise12/image_test19.npy  \n",
            "  inflating: datasets-promise12/image_test18.npy  \n",
            "  inflating: datasets-promise12/image_test24.npy  \n",
            "  inflating: datasets-promise12/image_train13.npy  \n",
            "  inflating: datasets-promise12/image_train07.npy  \n",
            "  inflating: datasets-promise12/label_train48.npy  \n",
            "  inflating: datasets-promise12/image_train39.npy  \n",
            "  inflating: datasets-promise12/image_train11.npy  \n",
            "  inflating: datasets-promise12/image_train05.npy  \n",
            "  inflating: datasets-promise12/image_test26.npy  \n",
            "  inflating: datasets-promise12/image_test27.npy  \n",
            "  inflating: datasets-promise12/image_train04.npy  \n",
            "  inflating: datasets-promise12/image_train10.npy  \n",
            "  inflating: datasets-promise12/image_train38.npy  \n",
            "  inflating: datasets-promise12/label_train49.npy  \n",
            "  inflating: datasets-promise12/image_train14.npy  \n",
            "  inflating: datasets-promise12/image_train00.npy  \n",
            "  inflating: datasets-promise12/image_train28.npy  \n",
            "  inflating: datasets-promise12/image_test23.npy  \n",
            "  inflating: datasets-promise12/image_test22.npy  \n",
            "  inflating: datasets-promise12/image_train29.npy  \n",
            "  inflating: datasets-promise12/image_train01.npy  \n",
            "  inflating: datasets-promise12/image_train15.npy  \n",
            "  inflating: datasets-promise12/image_train03.npy  \n",
            "  inflating: datasets-promise12/image_train17.npy  \n",
            "  inflating: datasets-promise12/image_test08.npy  \n",
            "  inflating: datasets-promise12/image_test20.npy  \n",
            "  inflating: datasets-promise12/image_test21.npy  \n",
            "  inflating: datasets-promise12/image_test09.npy  \n",
            "  inflating: datasets-promise12/image_train16.npy  \n",
            "  inflating: datasets-promise12/image_train02.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ntiCWHbUNVl",
        "colab_type": "text"
      },
      "source": [
        "Set Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnJtvsUsOAQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "path_to_data = 'DATA_PATH'\n",
        "path_to_save = 'RESULT_PATH'\n",
        "\n",
        "N= 50\n",
        "N_test= 30\n",
        "\n",
        "img_width= 128\n",
        "img_height= 128\n",
        "img_thickness= 32\n",
        "img_channels= 1\n",
        "\n",
        "learning_rate= float(5e-4)\n",
        "epochs= 150\n",
        "val_size= float(1/9)\n",
        "test_size= 0.1\n",
        "dropout= 0.5\n",
        "batch_size= 4\n",
        "patience= 150\n",
        "\n",
        "#f= [0, 2, 4, 8, 16, 32, 16, 8, 4, 2]\n",
        "#f= [0, 4, 8, 16, 32, 64, 32, 16, 8, 4]\n",
        "#f= [0, 8, 16, 32, 64, 128, 64, 32, 16, 8]\n",
        "num_channels = [0, 16, 32, 64, 128, 256, 128, 64, 32, 16]\n",
        "f = num_channels\n",
        "#f= [0, 32, 64, 128, 256, 512, 256, 128, 64, 32]\n",
        "#f= [0, 64, 128, 256, 512, 1024, 512, 256, 128, 64]\n",
        "\n",
        "#f= [0, 128, 256, 512, 1024, 2048, 1024, 512, 256, 128]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dwrPF5JVHQ9",
        "colab_type": "text"
      },
      "source": [
        "Parse Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98LngX5YOGO7",
        "colab_type": "code",
        "outputId": "595b27d5-065c-42da-e614-7eb0a3fed621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Load Training Data\n",
        "X_train = np.zeros((N, img_thickness, img_height, img_width, img_channels), dtype=np.float32)\n",
        "Y_train = np.zeros((N, img_thickness, img_height, img_width, img_channels), dtype=np.float32)\n",
        "\n",
        "print('','','')\n",
        "print('','','')\n",
        "print('Loading Training Data')\n",
        "\n",
        "for n in tqdm(range(N)):\n",
        "    image = np.load(os.path.join(DATA_PATH, \"image_train%02d.npy\" % n))\n",
        "    label = np.load(os.path.join(DATA_PATH, \"label_train%02d.npy\" % n))\n",
        "    X_train[n] = image[:,:,:,np.newaxis]\n",
        "    Y_train[n] = label[:,:,:,np.newaxis]\n",
        "\n",
        "print('Loaded Training Data')\n",
        "tf.print(X_train.shape)\n",
        "X_train = X_train[:int(X_train.shape[0]*(1-test_size))]\n",
        "Y_train = Y_train[:int(Y_train.shape[0]*(1-test_size))]\n",
        "tf.print(X_train.shape)\n",
        "\n",
        "# Load Testing Data\n",
        "X_test = np.zeros((N_test, img_thickness, img_height, img_width, img_channels), dtype=np.float32)\n",
        "\n",
        "print('','','')\n",
        "print('','','')\n",
        "print('Loading Testing Data')\n",
        "\n",
        "for n in tqdm(range(N_test)):\n",
        "    image = np.load(os.path.join(DATA_PATH, \"image_test%02d.npy\" % n))\n",
        "    X_test[n] = image[:,:,:,np.newaxis]\n",
        "\n",
        "print('Loaded Testing Data')\n",
        "print('','','')\n",
        "print('','','')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 664.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  \n",
            "  \n",
            "Loading Training Data\n",
            "Loaded Training Data\n",
            "(50, 32, 128, 128, 1)\n",
            "(45, 32, 128, 128, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 30/30 [00:00<00:00, 1098.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  \n",
            "  \n",
            "Loading Testing Data\n",
            "Loaded Testing Data\n",
            "  \n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj15fI63agNY",
        "colab_type": "text"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX7vZv9cahoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1.0):\n",
        "\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (\n",
        "        K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlPNlKqXaYXR",
        "colab_type": "text"
      },
      "source": [
        "U-Net Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26TFNZY0YeTL",
        "colab_type": "code",
        "outputId": "4564a2f8-b5e0-45b7-de29-f9a4f6960ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# UNet Model\n",
        "inputs = tf.keras.layers.Input((img_thickness, img_width, img_height, img_channels))\n",
        "\n",
        "# Convert integers in image matrix to floating point \n",
        "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "\n",
        "# Encoding\n",
        "c1 = tf.keras.layers.Conv3D(f[1], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(s)\n",
        "c1 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c1)\n",
        "c1 = tf.keras.layers.Dropout(dropout)(c1)\n",
        "c2 = tf.keras.layers.Conv3D(f[1], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c1)\n",
        "p1 = tf.keras.layers.MaxPooling3D((2,2,2))(c1)\n",
        "\n",
        "\n",
        "c2 = tf.keras.layers.Conv3D(f[2], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(p1)\n",
        "c2 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c2)\n",
        "c2 = tf.keras.layers.Dropout(dropout)(c2)\n",
        "c2 = tf.keras.layers.Conv3D(f[2], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c2)\n",
        "p2 = tf.keras.layers.MaxPooling3D((2,2,2))(c2)\n",
        "\n",
        "\n",
        "c3 = tf.keras.layers.Conv3D(f[3], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(p2)\n",
        "c3 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c3)\n",
        "c3 = tf.keras.layers.Dropout(dropout)(c3)\n",
        "c3 = tf.keras.layers.Conv3D(f[3], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c3)\n",
        "p3 = tf.keras.layers.MaxPooling3D((2,2,2))(c3)\n",
        "\n",
        "\n",
        "c4 = tf.keras.layers.Conv3D(f[4], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(p3)\n",
        "c4 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c4)\n",
        "c4 = tf.keras.layers.Dropout(dropout)(c4)\n",
        "c4 = tf.keras.layers.Conv3D(f[4], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c4)\n",
        "p4 = tf.keras.layers.MaxPooling3D((2,2,2))(c4)\n",
        "\n",
        "\n",
        "c5 = tf.keras.layers.Conv3D(f[5], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(p4)\n",
        "c5 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c5)\n",
        "c5 = tf.keras.layers.Dropout(dropout)(c5)\n",
        "c5 = tf.keras.layers.Conv3D(f[5], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c5)\n",
        "p5 = tf.keras.layers.MaxPooling3D((2,2,2))(c5)\n",
        "\n",
        "# Decoding Layers\n",
        "u6 = tf.keras.layers.Conv3DTranspose(f[6], (2, 2, 2), strides=(2, 2, 2), padding='same',)(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "c6 = tf.keras.layers.Conv3D(f[6], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(u6)\n",
        "c6 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c6)\n",
        "c6 = tf.keras.layers.Dropout(dropout)(c6)\n",
        "c6 = tf.keras.layers.Conv3D(f[6], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "\n",
        "u7 = tf.keras.layers.Conv3DTranspose(f[7], (2, 2, 2), strides=(2, 2, 2), padding='same',)(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "c7 = tf.keras.layers.Conv3D(f[7], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(u7)\n",
        "c7 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c7)\n",
        "c7 = tf.keras.layers.Dropout(dropout)(c7)\n",
        "c7 = tf.keras.layers.Conv3D(f[7], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "\n",
        "u8 = tf.keras.layers.Conv3DTranspose(f[8], (2, 2, 2), strides=(2, 2, 2), padding='same',)(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "c8 = tf.keras.layers.Conv3D(f[8], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(u8)\n",
        "c8 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c8)\n",
        "c8 = tf.keras.layers.Dropout(dropout)(c8)\n",
        "c8 = tf.keras.layers.Conv3D(f[8], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "\n",
        "u9 = tf.keras.layers.Conv3DTranspose(f[9], (2, 2, 2), strides=(2, 2, 2), padding='same',)(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, c1])\n",
        "c9 = tf.keras.layers.Conv3D(f[9], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(u9)\n",
        "c9 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c9)\n",
        "c9 = tf.keras.layers.Dropout(dropout)(c9)\n",
        "c9 = tf.keras.layers.Conv3D(f[9], (3, 3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "\n",
        "outputs = tf.keras.layers.Conv3D(1, (1, 1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate), loss=dice_coef_loss, metrics=[dice_coef])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint('saved_model/best_model.h5',\n",
        "                                                 verbose=1, save_best_only=True),\n",
        "            tf.keras.callbacks.EarlyStopping(patience=patience, monitor='loss'),\n",
        "            tf.keras.callbacks.TensorBoard(log_dir='logs')]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 128, 128 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 32, 128, 128, 0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_76 (Conv3D)              (None, 32, 128, 128, 448         lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 32, 128, 128, 64          conv3d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 32, 128, 128, 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_20 (MaxPooling3D) (None, 16, 64, 64, 1 0           dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_78 (Conv3D)              (None, 16, 64, 64, 3 13856       max_pooling3d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 16, 64, 64, 3 128         conv3d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 16, 64, 64, 3 0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_79 (Conv3D)              (None, 16, 64, 64, 3 27680       dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_21 (MaxPooling3D) (None, 8, 32, 32, 32 0           conv3d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_80 (Conv3D)              (None, 8, 32, 32, 64 55360       max_pooling3d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 32, 32, 64 256         conv3d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 32, 32, 64 0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_81 (Conv3D)              (None, 8, 32, 32, 64 110656      dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_22 (MaxPooling3D) (None, 4, 16, 16, 64 0           conv3d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_82 (Conv3D)              (None, 4, 16, 16, 12 221312      max_pooling3d_22[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 4, 16, 16, 12 512         conv3d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 4, 16, 16, 12 0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_83 (Conv3D)              (None, 4, 16, 16, 12 442496      dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_23 (MaxPooling3D) (None, 2, 8, 8, 128) 0           conv3d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_84 (Conv3D)              (None, 2, 8, 8, 256) 884992      max_pooling3d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 2, 8, 8, 256) 1024        conv3d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 2, 8, 8, 256) 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_85 (Conv3D)              (None, 2, 8, 8, 256) 1769728     dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_16 (Conv3DTran (None, 4, 16, 16, 12 262272      conv3d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 4, 16, 16, 25 0           conv3d_transpose_16[0][0]        \n",
            "                                                                 conv3d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_86 (Conv3D)              (None, 4, 16, 16, 12 884864      concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 16, 16, 12 512         conv3d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 4, 16, 16, 12 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_87 (Conv3D)              (None, 4, 16, 16, 12 442496      dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_17 (Conv3DTran (None, 8, 32, 32, 64 65600       conv3d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 8, 32, 32, 12 0           conv3d_transpose_17[0][0]        \n",
            "                                                                 conv3d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_88 (Conv3D)              (None, 8, 32, 32, 64 221248      concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 8, 32, 32, 64 256         conv3d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 8, 32, 32, 64 0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_89 (Conv3D)              (None, 8, 32, 32, 64 110656      dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_18 (Conv3DTran (None, 16, 64, 64, 3 16416       conv3d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 64, 64, 6 0           conv3d_transpose_18[0][0]        \n",
            "                                                                 conv3d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_90 (Conv3D)              (None, 16, 64, 64, 3 55328       concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 64, 64, 3 128         conv3d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 16, 64, 64, 3 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_91 (Conv3D)              (None, 16, 64, 64, 3 27680       dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_19 (Conv3DTran (None, 32, 128, 128, 4112        conv3d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 32, 128, 128, 0           conv3d_transpose_19[0][0]        \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_92 (Conv3D)              (None, 32, 128, 128, 13840       concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 32, 128, 128, 64          conv3d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 32, 128, 128, 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_93 (Conv3D)              (None, 32, 128, 128, 6928        dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_94 (Conv3D)              (None, 32, 128, 128, 17          conv3d_93[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 5,640,929\n",
            "Trainable params: 5,639,457\n",
            "Non-trainable params: 1,472\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy8_ZRB9baL1",
        "colab_type": "text"
      },
      "source": [
        "Train the 3D U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhN9Dv0faeSg",
        "colab_type": "code",
        "outputId": "e75808f6-1c15-4c75-b426-38024608b18a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results = model.fit(X_train, Y_train, validation_split=val_size, batch_size=batch_size,\n",
        "                    epochs=epochs, callbacks=callbacks) \n",
        "model.save('saved_model/final_model.h5') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.0732 - dice_coef: 0.0732\n",
            "Epoch 00001: val_loss improved from inf to -0.04834, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 7s 682ms/step - loss: -0.0732 - dice_coef: 0.0732 - val_loss: -0.0483 - val_dice_coef: 0.0483\n",
            "Epoch 2/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.1006 - dice_coef: 0.1006\n",
            "Epoch 00002: val_loss did not improve from -0.04834\n",
            "10/10 [==============================] - 6s 601ms/step - loss: -0.1006 - dice_coef: 0.1006 - val_loss: -0.0456 - val_dice_coef: 0.0456\n",
            "Epoch 3/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.1407 - dice_coef: 0.1407\n",
            "Epoch 00003: val_loss improved from -0.04834 to -0.05020, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 627ms/step - loss: -0.1407 - dice_coef: 0.1407 - val_loss: -0.0502 - val_dice_coef: 0.0502\n",
            "Epoch 4/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.2137 - dice_coef: 0.2137\n",
            "Epoch 00004: val_loss improved from -0.05020 to -0.11932, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: -0.2137 - dice_coef: 0.2137 - val_loss: -0.1193 - val_dice_coef: 0.1193\n",
            "Epoch 5/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.2764 - dice_coef: 0.2764\n",
            "Epoch 00005: val_loss did not improve from -0.11932\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.2764 - dice_coef: 0.2764 - val_loss: -0.0880 - val_dice_coef: 0.0880\n",
            "Epoch 6/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.3475 - dice_coef: 0.3475\n",
            "Epoch 00006: val_loss improved from -0.11932 to -0.38591, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 630ms/step - loss: -0.3475 - dice_coef: 0.3475 - val_loss: -0.3859 - val_dice_coef: 0.3859\n",
            "Epoch 7/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.4085 - dice_coef: 0.4085\n",
            "Epoch 00007: val_loss did not improve from -0.38591\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.4085 - dice_coef: 0.4085 - val_loss: -0.2394 - val_dice_coef: 0.2394\n",
            "Epoch 8/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.4815 - dice_coef: 0.4815\n",
            "Epoch 00008: val_loss improved from -0.38591 to -0.58268, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 631ms/step - loss: -0.4815 - dice_coef: 0.4815 - val_loss: -0.5827 - val_dice_coef: 0.5827\n",
            "Epoch 9/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.5789 - dice_coef: 0.5789\n",
            "Epoch 00009: val_loss improved from -0.58268 to -0.59618, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 632ms/step - loss: -0.5789 - dice_coef: 0.5789 - val_loss: -0.5962 - val_dice_coef: 0.5962\n",
            "Epoch 10/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.6398 - dice_coef: 0.6398\n",
            "Epoch 00010: val_loss did not improve from -0.59618\n",
            "10/10 [==============================] - 6s 607ms/step - loss: -0.6398 - dice_coef: 0.6398 - val_loss: -0.5908 - val_dice_coef: 0.5908\n",
            "Epoch 11/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.6542 - dice_coef: 0.6542\n",
            "Epoch 00011: val_loss did not improve from -0.59618\n",
            "10/10 [==============================] - 6s 608ms/step - loss: -0.6542 - dice_coef: 0.6542 - val_loss: -0.4209 - val_dice_coef: 0.4209\n",
            "Epoch 12/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.6964 - dice_coef: 0.6964\n",
            "Epoch 00012: val_loss did not improve from -0.59618\n",
            "10/10 [==============================] - 6s 607ms/step - loss: -0.6964 - dice_coef: 0.6964 - val_loss: -0.5010 - val_dice_coef: 0.5010\n",
            "Epoch 13/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7405 - dice_coef: 0.7405\n",
            "Epoch 00013: val_loss did not improve from -0.59618\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.7405 - dice_coef: 0.7405 - val_loss: -6.9048e-05 - val_dice_coef: 6.9048e-05\n",
            "Epoch 14/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7619 - dice_coef: 0.7619\n",
            "Epoch 00014: val_loss improved from -0.59618 to -0.70797, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 627ms/step - loss: -0.7619 - dice_coef: 0.7619 - val_loss: -0.7080 - val_dice_coef: 0.7080\n",
            "Epoch 15/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7495 - dice_coef: 0.7495\n",
            "Epoch 00015: val_loss improved from -0.70797 to -0.73429, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 629ms/step - loss: -0.7495 - dice_coef: 0.7495 - val_loss: -0.7343 - val_dice_coef: 0.7343\n",
            "Epoch 16/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7516 - dice_coef: 0.7516\n",
            "Epoch 00016: val_loss did not improve from -0.73429\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.7516 - dice_coef: 0.7516 - val_loss: -0.6980 - val_dice_coef: 0.6980\n",
            "Epoch 17/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7698 - dice_coef: 0.7698\n",
            "Epoch 00017: val_loss did not improve from -0.73429\n",
            "10/10 [==============================] - 6s 602ms/step - loss: -0.7698 - dice_coef: 0.7698 - val_loss: -0.1440 - val_dice_coef: 0.1440\n",
            "Epoch 18/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7832 - dice_coef: 0.7832\n",
            "Epoch 00018: val_loss did not improve from -0.73429\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.7832 - dice_coef: 0.7832 - val_loss: -0.4027 - val_dice_coef: 0.4027\n",
            "Epoch 19/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7844 - dice_coef: 0.7844\n",
            "Epoch 00019: val_loss did not improve from -0.73429\n",
            "10/10 [==============================] - 6s 608ms/step - loss: -0.7844 - dice_coef: 0.7844 - val_loss: -0.7041 - val_dice_coef: 0.7041\n",
            "Epoch 20/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8000 - dice_coef: 0.8000\n",
            "Epoch 00020: val_loss did not improve from -0.73429\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.8000 - dice_coef: 0.8000 - val_loss: -0.7273 - val_dice_coef: 0.7273\n",
            "Epoch 21/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8024 - dice_coef: 0.8024\n",
            "Epoch 00021: val_loss did not improve from -0.73429\n",
            "10/10 [==============================] - 6s 607ms/step - loss: -0.8024 - dice_coef: 0.8024 - val_loss: -0.6182 - val_dice_coef: 0.6182\n",
            "Epoch 22/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8076 - dice_coef: 0.8076\n",
            "Epoch 00022: val_loss did not improve from -0.73429\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8076 - dice_coef: 0.8076 - val_loss: -0.7106 - val_dice_coef: 0.7106\n",
            "Epoch 23/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8259 - dice_coef: 0.8259\n",
            "Epoch 00023: val_loss improved from -0.73429 to -0.74597, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: -0.8259 - dice_coef: 0.8259 - val_loss: -0.7460 - val_dice_coef: 0.7460\n",
            "Epoch 24/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8317 - dice_coef: 0.8317\n",
            "Epoch 00024: val_loss did not improve from -0.74597\n",
            "10/10 [==============================] - 6s 602ms/step - loss: -0.8317 - dice_coef: 0.8317 - val_loss: -0.1865 - val_dice_coef: 0.1865\n",
            "Epoch 25/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8171 - dice_coef: 0.8171\n",
            "Epoch 00025: val_loss did not improve from -0.74597\n",
            "10/10 [==============================] - 6s 602ms/step - loss: -0.8171 - dice_coef: 0.8171 - val_loss: -0.0612 - val_dice_coef: 0.0612\n",
            "Epoch 26/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8059 - dice_coef: 0.8059\n",
            "Epoch 00026: val_loss did not improve from -0.74597\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.8059 - dice_coef: 0.8059 - val_loss: -0.7110 - val_dice_coef: 0.7110\n",
            "Epoch 27/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8202 - dice_coef: 0.8202\n",
            "Epoch 00027: val_loss did not improve from -0.74597\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8202 - dice_coef: 0.8202 - val_loss: -0.6205 - val_dice_coef: 0.6205\n",
            "Epoch 28/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8220 - dice_coef: 0.8220\n",
            "Epoch 00028: val_loss did not improve from -0.74597\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8220 - dice_coef: 0.8220 - val_loss: -0.0221 - val_dice_coef: 0.0221\n",
            "Epoch 29/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8319 - dice_coef: 0.8319\n",
            "Epoch 00029: val_loss improved from -0.74597 to -0.75709, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 631ms/step - loss: -0.8319 - dice_coef: 0.8319 - val_loss: -0.7571 - val_dice_coef: 0.7571\n",
            "Epoch 30/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8489 - dice_coef: 0.8489\n",
            "Epoch 00030: val_loss did not improve from -0.75709\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8489 - dice_coef: 0.8489 - val_loss: -0.7176 - val_dice_coef: 0.7176\n",
            "Epoch 31/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8394 - dice_coef: 0.8394\n",
            "Epoch 00031: val_loss improved from -0.75709 to -0.76410, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 629ms/step - loss: -0.8394 - dice_coef: 0.8394 - val_loss: -0.7641 - val_dice_coef: 0.7641\n",
            "Epoch 32/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8652 - dice_coef: 0.8652\n",
            "Epoch 00032: val_loss improved from -0.76410 to -0.76656, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: -0.8652 - dice_coef: 0.8652 - val_loss: -0.7666 - val_dice_coef: 0.7666\n",
            "Epoch 33/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8656 - dice_coef: 0.8656\n",
            "Epoch 00033: val_loss did not improve from -0.76656\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.8656 - dice_coef: 0.8656 - val_loss: -0.7579 - val_dice_coef: 0.7579\n",
            "Epoch 34/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8660 - dice_coef: 0.8660\n",
            "Epoch 00034: val_loss did not improve from -0.76656\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8660 - dice_coef: 0.8660 - val_loss: -0.7563 - val_dice_coef: 0.7563\n",
            "Epoch 35/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8677 - dice_coef: 0.8677\n",
            "Epoch 00035: val_loss improved from -0.76656 to -0.77535, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 631ms/step - loss: -0.8677 - dice_coef: 0.8677 - val_loss: -0.7754 - val_dice_coef: 0.7754\n",
            "Epoch 36/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8680 - dice_coef: 0.8680\n",
            "Epoch 00036: val_loss improved from -0.77535 to -0.78061, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 629ms/step - loss: -0.8680 - dice_coef: 0.8680 - val_loss: -0.7806 - val_dice_coef: 0.7806\n",
            "Epoch 37/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8773 - dice_coef: 0.8773\n",
            "Epoch 00037: val_loss improved from -0.78061 to -0.79562, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: -0.8773 - dice_coef: 0.8773 - val_loss: -0.7956 - val_dice_coef: 0.7956\n",
            "Epoch 38/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8712 - dice_coef: 0.8712\n",
            "Epoch 00038: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 608ms/step - loss: -0.8712 - dice_coef: 0.8712 - val_loss: -0.7373 - val_dice_coef: 0.7373\n",
            "Epoch 39/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8647 - dice_coef: 0.8647\n",
            "Epoch 00039: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 609ms/step - loss: -0.8647 - dice_coef: 0.8647 - val_loss: -0.7527 - val_dice_coef: 0.7527\n",
            "Epoch 40/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8740 - dice_coef: 0.8740\n",
            "Epoch 00040: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.8740 - dice_coef: 0.8740 - val_loss: -0.7348 - val_dice_coef: 0.7348\n",
            "Epoch 41/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8576 - dice_coef: 0.8576\n",
            "Epoch 00041: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.8576 - dice_coef: 0.8576 - val_loss: -0.7849 - val_dice_coef: 0.7849\n",
            "Epoch 42/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8726 - dice_coef: 0.8726\n",
            "Epoch 00042: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.8726 - dice_coef: 0.8726 - val_loss: -0.7907 - val_dice_coef: 0.7907\n",
            "Epoch 43/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8790 - dice_coef: 0.8790\n",
            "Epoch 00043: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8790 - dice_coef: 0.8790 - val_loss: -0.7795 - val_dice_coef: 0.7795\n",
            "Epoch 44/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8785 - dice_coef: 0.8785\n",
            "Epoch 00044: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.8785 - dice_coef: 0.8785 - val_loss: -0.7527 - val_dice_coef: 0.7527\n",
            "Epoch 45/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8784 - dice_coef: 0.8784\n",
            "Epoch 00045: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.8784 - dice_coef: 0.8784 - val_loss: -0.7695 - val_dice_coef: 0.7695\n",
            "Epoch 46/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8886 - dice_coef: 0.8886\n",
            "Epoch 00046: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8886 - dice_coef: 0.8886 - val_loss: -0.7672 - val_dice_coef: 0.7672\n",
            "Epoch 47/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8859 - dice_coef: 0.8859\n",
            "Epoch 00047: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.8859 - dice_coef: 0.8859 - val_loss: -0.7880 - val_dice_coef: 0.7880\n",
            "Epoch 48/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8920 - dice_coef: 0.8920\n",
            "Epoch 00048: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8920 - dice_coef: 0.8920 - val_loss: -0.7695 - val_dice_coef: 0.7695\n",
            "Epoch 49/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8861 - dice_coef: 0.8861\n",
            "Epoch 00049: val_loss did not improve from -0.79562\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.8861 - dice_coef: 0.8861 - val_loss: -0.7913 - val_dice_coef: 0.7913\n",
            "Epoch 50/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8875 - dice_coef: 0.8875\n",
            "Epoch 00050: val_loss improved from -0.79562 to -0.79582, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: -0.8875 - dice_coef: 0.8875 - val_loss: -0.7958 - val_dice_coef: 0.7958\n",
            "Epoch 51/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8903 - dice_coef: 0.8903\n",
            "Epoch 00051: val_loss did not improve from -0.79582\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.8903 - dice_coef: 0.8903 - val_loss: -0.7287 - val_dice_coef: 0.7287\n",
            "Epoch 52/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8866 - dice_coef: 0.8866\n",
            "Epoch 00052: val_loss did not improve from -0.79582\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.8866 - dice_coef: 0.8866 - val_loss: -0.7680 - val_dice_coef: 0.7680\n",
            "Epoch 53/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8938 - dice_coef: 0.8938\n",
            "Epoch 00053: val_loss did not improve from -0.79582\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.8938 - dice_coef: 0.8938 - val_loss: -0.7877 - val_dice_coef: 0.7877\n",
            "Epoch 54/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8954 - dice_coef: 0.8954\n",
            "Epoch 00054: val_loss did not improve from -0.79582\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8954 - dice_coef: 0.8954 - val_loss: -0.7928 - val_dice_coef: 0.7928\n",
            "Epoch 55/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9007 - dice_coef: 0.9007\n",
            "Epoch 00055: val_loss did not improve from -0.79582\n",
            "10/10 [==============================] - 6s 602ms/step - loss: -0.9007 - dice_coef: 0.9007 - val_loss: -0.7918 - val_dice_coef: 0.7918\n",
            "Epoch 56/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8972 - dice_coef: 0.8972\n",
            "Epoch 00056: val_loss improved from -0.79582 to -0.80539, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: -0.8972 - dice_coef: 0.8972 - val_loss: -0.8054 - val_dice_coef: 0.8054\n",
            "Epoch 57/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8928 - dice_coef: 0.8928\n",
            "Epoch 00057: val_loss did not improve from -0.80539\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8928 - dice_coef: 0.8928 - val_loss: -0.7744 - val_dice_coef: 0.7744\n",
            "Epoch 58/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8940 - dice_coef: 0.8940\n",
            "Epoch 00058: val_loss did not improve from -0.80539\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.8940 - dice_coef: 0.8940 - val_loss: -0.7955 - val_dice_coef: 0.7955\n",
            "Epoch 59/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9038 - dice_coef: 0.9038\n",
            "Epoch 00059: val_loss did not improve from -0.80539\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.9038 - dice_coef: 0.9038 - val_loss: -0.7632 - val_dice_coef: 0.7632\n",
            "Epoch 60/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9046 - dice_coef: 0.9046\n",
            "Epoch 00060: val_loss did not improve from -0.80539\n",
            "10/10 [==============================] - 6s 607ms/step - loss: -0.9046 - dice_coef: 0.9046 - val_loss: -0.7946 - val_dice_coef: 0.7946\n",
            "Epoch 61/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9078 - dice_coef: 0.9078\n",
            "Epoch 00061: val_loss did not improve from -0.80539\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9078 - dice_coef: 0.9078 - val_loss: -0.7913 - val_dice_coef: 0.7913\n",
            "Epoch 62/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9074 - dice_coef: 0.9074\n",
            "Epoch 00062: val_loss did not improve from -0.80539\n",
            "10/10 [==============================] - 6s 608ms/step - loss: -0.9074 - dice_coef: 0.9074 - val_loss: -0.7852 - val_dice_coef: 0.7852\n",
            "Epoch 63/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9024 - dice_coef: 0.9024\n",
            "Epoch 00063: val_loss improved from -0.80539 to -0.82197, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 632ms/step - loss: -0.9024 - dice_coef: 0.9024 - val_loss: -0.8220 - val_dice_coef: 0.8220\n",
            "Epoch 64/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9074 - dice_coef: 0.9074\n",
            "Epoch 00064: val_loss did not improve from -0.82197\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9074 - dice_coef: 0.9074 - val_loss: -0.8171 - val_dice_coef: 0.8171\n",
            "Epoch 65/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9050 - dice_coef: 0.9050\n",
            "Epoch 00065: val_loss did not improve from -0.82197\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9050 - dice_coef: 0.9050 - val_loss: -0.8025 - val_dice_coef: 0.8025\n",
            "Epoch 66/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9074 - dice_coef: 0.9074\n",
            "Epoch 00066: val_loss did not improve from -0.82197\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9074 - dice_coef: 0.9074 - val_loss: -0.8068 - val_dice_coef: 0.8068\n",
            "Epoch 67/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9067 - dice_coef: 0.9067\n",
            "Epoch 00067: val_loss did not improve from -0.82197\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9067 - dice_coef: 0.9067 - val_loss: -0.7910 - val_dice_coef: 0.7910\n",
            "Epoch 68/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9098 - dice_coef: 0.9098\n",
            "Epoch 00068: val_loss did not improve from -0.82197\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9098 - dice_coef: 0.9098 - val_loss: -0.7857 - val_dice_coef: 0.7857\n",
            "Epoch 69/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9130 - dice_coef: 0.9130\n",
            "Epoch 00069: val_loss did not improve from -0.82197\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9130 - dice_coef: 0.9130 - val_loss: -0.8161 - val_dice_coef: 0.8161\n",
            "Epoch 70/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9124 - dice_coef: 0.9124\n",
            "Epoch 00070: val_loss did not improve from -0.82197\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9124 - dice_coef: 0.9124 - val_loss: -0.7966 - val_dice_coef: 0.7966\n",
            "Epoch 71/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9052 - dice_coef: 0.9052\n",
            "Epoch 00071: val_loss did not improve from -0.82197\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9052 - dice_coef: 0.9052 - val_loss: -0.8121 - val_dice_coef: 0.8121\n",
            "Epoch 72/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9117 - dice_coef: 0.9117\n",
            "Epoch 00072: val_loss did not improve from -0.82197\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.9117 - dice_coef: 0.9117 - val_loss: -0.8201 - val_dice_coef: 0.8201\n",
            "Epoch 73/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9100 - dice_coef: 0.9100\n",
            "Epoch 00073: val_loss improved from -0.82197 to -0.82811, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 627ms/step - loss: -0.9100 - dice_coef: 0.9100 - val_loss: -0.8281 - val_dice_coef: 0.8281\n",
            "Epoch 74/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9090 - dice_coef: 0.9090\n",
            "Epoch 00074: val_loss did not improve from -0.82811\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9090 - dice_coef: 0.9090 - val_loss: -0.8127 - val_dice_coef: 0.8127\n",
            "Epoch 75/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9093 - dice_coef: 0.9093\n",
            "Epoch 00075: val_loss did not improve from -0.82811\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9093 - dice_coef: 0.9093 - val_loss: -0.8143 - val_dice_coef: 0.8143\n",
            "Epoch 76/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9175 - dice_coef: 0.9175\n",
            "Epoch 00076: val_loss did not improve from -0.82811\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9175 - dice_coef: 0.9175 - val_loss: -0.8083 - val_dice_coef: 0.8083\n",
            "Epoch 77/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9140 - dice_coef: 0.9140\n",
            "Epoch 00077: val_loss did not improve from -0.82811\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.9140 - dice_coef: 0.9140 - val_loss: -0.8058 - val_dice_coef: 0.8058\n",
            "Epoch 78/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9161 - dice_coef: 0.9161\n",
            "Epoch 00078: val_loss did not improve from -0.82811\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9161 - dice_coef: 0.9161 - val_loss: -0.8184 - val_dice_coef: 0.8184\n",
            "Epoch 79/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9154 - dice_coef: 0.9154\n",
            "Epoch 00079: val_loss did not improve from -0.82811\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9154 - dice_coef: 0.9154 - val_loss: -0.8221 - val_dice_coef: 0.8221\n",
            "Epoch 80/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9168 - dice_coef: 0.9168\n",
            "Epoch 00080: val_loss improved from -0.82811 to -0.82946, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 624ms/step - loss: -0.9168 - dice_coef: 0.9168 - val_loss: -0.8295 - val_dice_coef: 0.8295\n",
            "Epoch 81/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9173 - dice_coef: 0.9173\n",
            "Epoch 00081: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9173 - dice_coef: 0.9173 - val_loss: -0.8259 - val_dice_coef: 0.8259\n",
            "Epoch 82/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9183 - dice_coef: 0.9183\n",
            "Epoch 00082: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 607ms/step - loss: -0.9183 - dice_coef: 0.9183 - val_loss: -0.8194 - val_dice_coef: 0.8194\n",
            "Epoch 83/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9187 - dice_coef: 0.9187\n",
            "Epoch 00083: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9187 - dice_coef: 0.9187 - val_loss: -0.8006 - val_dice_coef: 0.8006\n",
            "Epoch 84/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9191 - dice_coef: 0.9191\n",
            "Epoch 00084: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9191 - dice_coef: 0.9191 - val_loss: -0.8052 - val_dice_coef: 0.8052\n",
            "Epoch 85/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9202 - dice_coef: 0.9202\n",
            "Epoch 00085: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9202 - dice_coef: 0.9202 - val_loss: -0.8067 - val_dice_coef: 0.8067\n",
            "Epoch 86/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9180 - dice_coef: 0.9180\n",
            "Epoch 00086: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9180 - dice_coef: 0.9180 - val_loss: -0.8142 - val_dice_coef: 0.8142\n",
            "Epoch 87/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9155 - dice_coef: 0.9155\n",
            "Epoch 00087: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9155 - dice_coef: 0.9155 - val_loss: -0.8188 - val_dice_coef: 0.8188\n",
            "Epoch 88/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9218 - dice_coef: 0.9218\n",
            "Epoch 00088: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9218 - dice_coef: 0.9218 - val_loss: -0.7885 - val_dice_coef: 0.7885\n",
            "Epoch 89/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9173 - dice_coef: 0.9173\n",
            "Epoch 00089: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9173 - dice_coef: 0.9173 - val_loss: -0.7894 - val_dice_coef: 0.7894\n",
            "Epoch 90/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9209 - dice_coef: 0.9209\n",
            "Epoch 00090: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 607ms/step - loss: -0.9209 - dice_coef: 0.9209 - val_loss: -0.8264 - val_dice_coef: 0.8264\n",
            "Epoch 91/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9205 - dice_coef: 0.9205\n",
            "Epoch 00091: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 602ms/step - loss: -0.9205 - dice_coef: 0.9205 - val_loss: -0.8046 - val_dice_coef: 0.8046\n",
            "Epoch 92/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9185 - dice_coef: 0.9185\n",
            "Epoch 00092: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9185 - dice_coef: 0.9185 - val_loss: -0.8048 - val_dice_coef: 0.8048\n",
            "Epoch 93/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9194 - dice_coef: 0.9194\n",
            "Epoch 00093: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 607ms/step - loss: -0.9194 - dice_coef: 0.9194 - val_loss: -0.8154 - val_dice_coef: 0.8154\n",
            "Epoch 94/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9223 - dice_coef: 0.9223\n",
            "Epoch 00094: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9223 - dice_coef: 0.9223 - val_loss: -0.7968 - val_dice_coef: 0.7968\n",
            "Epoch 95/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9237 - dice_coef: 0.9237\n",
            "Epoch 00095: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9237 - dice_coef: 0.9237 - val_loss: -0.8055 - val_dice_coef: 0.8055\n",
            "Epoch 96/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9236 - dice_coef: 0.9236\n",
            "Epoch 00096: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.9236 - dice_coef: 0.9236 - val_loss: -0.7972 - val_dice_coef: 0.7972\n",
            "Epoch 97/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9223 - dice_coef: 0.9223\n",
            "Epoch 00097: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9223 - dice_coef: 0.9223 - val_loss: -0.8175 - val_dice_coef: 0.8175\n",
            "Epoch 98/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9270 - dice_coef: 0.9270\n",
            "Epoch 00098: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9270 - dice_coef: 0.9270 - val_loss: -0.8141 - val_dice_coef: 0.8141\n",
            "Epoch 99/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9238 - dice_coef: 0.9238\n",
            "Epoch 00099: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9238 - dice_coef: 0.9238 - val_loss: -0.8174 - val_dice_coef: 0.8174\n",
            "Epoch 100/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9240 - dice_coef: 0.9240\n",
            "Epoch 00100: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9240 - dice_coef: 0.9240 - val_loss: -0.8222 - val_dice_coef: 0.8222\n",
            "Epoch 101/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9249 - dice_coef: 0.9249\n",
            "Epoch 00101: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.9249 - dice_coef: 0.9249 - val_loss: -0.8198 - val_dice_coef: 0.8198\n",
            "Epoch 102/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9261 - dice_coef: 0.9261\n",
            "Epoch 00102: val_loss did not improve from -0.82946\n",
            "10/10 [==============================] - 6s 607ms/step - loss: -0.9261 - dice_coef: 0.9261 - val_loss: -0.8134 - val_dice_coef: 0.8134\n",
            "Epoch 103/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9255 - dice_coef: 0.9255\n",
            "Epoch 00103: val_loss improved from -0.82946 to -0.83788, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 629ms/step - loss: -0.9255 - dice_coef: 0.9255 - val_loss: -0.8379 - val_dice_coef: 0.8379\n",
            "Epoch 104/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9273 - dice_coef: 0.9273\n",
            "Epoch 00104: val_loss did not improve from -0.83788\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9273 - dice_coef: 0.9273 - val_loss: -0.8102 - val_dice_coef: 0.8102\n",
            "Epoch 105/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9284 - dice_coef: 0.9284\n",
            "Epoch 00105: val_loss did not improve from -0.83788\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9284 - dice_coef: 0.9284 - val_loss: -0.8287 - val_dice_coef: 0.8287\n",
            "Epoch 106/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9263 - dice_coef: 0.9263\n",
            "Epoch 00106: val_loss did not improve from -0.83788\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9263 - dice_coef: 0.9263 - val_loss: -0.8275 - val_dice_coef: 0.8275\n",
            "Epoch 107/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9310 - dice_coef: 0.9310\n",
            "Epoch 00107: val_loss did not improve from -0.83788\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9310 - dice_coef: 0.9310 - val_loss: -0.8329 - val_dice_coef: 0.8329\n",
            "Epoch 108/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9323 - dice_coef: 0.9323\n",
            "Epoch 00108: val_loss did not improve from -0.83788\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9323 - dice_coef: 0.9323 - val_loss: -0.8326 - val_dice_coef: 0.8326\n",
            "Epoch 109/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9295 - dice_coef: 0.9295\n",
            "Epoch 00109: val_loss improved from -0.83788 to -0.83995, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: -0.9295 - dice_coef: 0.9295 - val_loss: -0.8400 - val_dice_coef: 0.8400\n",
            "Epoch 110/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9301 - dice_coef: 0.9301\n",
            "Epoch 00110: val_loss did not improve from -0.83995\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9301 - dice_coef: 0.9301 - val_loss: -0.8257 - val_dice_coef: 0.8257\n",
            "Epoch 111/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9329 - dice_coef: 0.9329\n",
            "Epoch 00111: val_loss did not improve from -0.83995\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9329 - dice_coef: 0.9329 - val_loss: -0.8290 - val_dice_coef: 0.8290\n",
            "Epoch 112/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9332 - dice_coef: 0.9332\n",
            "Epoch 00112: val_loss did not improve from -0.83995\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9332 - dice_coef: 0.9332 - val_loss: -0.8378 - val_dice_coef: 0.8378\n",
            "Epoch 113/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9295 - dice_coef: 0.9295\n",
            "Epoch 00113: val_loss did not improve from -0.83995\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9295 - dice_coef: 0.9295 - val_loss: -0.8377 - val_dice_coef: 0.8377\n",
            "Epoch 114/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9321 - dice_coef: 0.9321\n",
            "Epoch 00114: val_loss did not improve from -0.83995\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9321 - dice_coef: 0.9321 - val_loss: -0.8220 - val_dice_coef: 0.8220\n",
            "Epoch 115/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9317 - dice_coef: 0.9317\n",
            "Epoch 00115: val_loss improved from -0.83995 to -0.84323, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: -0.9317 - dice_coef: 0.9317 - val_loss: -0.8432 - val_dice_coef: 0.8432\n",
            "Epoch 116/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9336 - dice_coef: 0.9336\n",
            "Epoch 00116: val_loss improved from -0.84323 to -0.84463, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: -0.9336 - dice_coef: 0.9336 - val_loss: -0.8446 - val_dice_coef: 0.8446\n",
            "Epoch 117/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9336 - dice_coef: 0.9336\n",
            "Epoch 00117: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9336 - dice_coef: 0.9336 - val_loss: -0.8400 - val_dice_coef: 0.8400\n",
            "Epoch 118/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9340 - dice_coef: 0.9340\n",
            "Epoch 00118: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 602ms/step - loss: -0.9340 - dice_coef: 0.9340 - val_loss: -0.8329 - val_dice_coef: 0.8329\n",
            "Epoch 119/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9332 - dice_coef: 0.9332\n",
            "Epoch 00119: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9332 - dice_coef: 0.9332 - val_loss: -0.8424 - val_dice_coef: 0.8424\n",
            "Epoch 120/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9329 - dice_coef: 0.9329\n",
            "Epoch 00120: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9329 - dice_coef: 0.9329 - val_loss: -0.8373 - val_dice_coef: 0.8373\n",
            "Epoch 121/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9365 - dice_coef: 0.9365\n",
            "Epoch 00121: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9365 - dice_coef: 0.9365 - val_loss: -0.8399 - val_dice_coef: 0.8399\n",
            "Epoch 122/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9340 - dice_coef: 0.9340\n",
            "Epoch 00122: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9340 - dice_coef: 0.9340 - val_loss: -0.8317 - val_dice_coef: 0.8317\n",
            "Epoch 123/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9331 - dice_coef: 0.9331\n",
            "Epoch 00123: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.9331 - dice_coef: 0.9331 - val_loss: -0.8433 - val_dice_coef: 0.8433\n",
            "Epoch 124/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9327 - dice_coef: 0.9327\n",
            "Epoch 00124: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9327 - dice_coef: 0.9327 - val_loss: -0.8401 - val_dice_coef: 0.8401\n",
            "Epoch 125/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9293 - dice_coef: 0.9293\n",
            "Epoch 00125: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9293 - dice_coef: 0.9293 - val_loss: -0.8409 - val_dice_coef: 0.8409\n",
            "Epoch 126/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9325 - dice_coef: 0.9325\n",
            "Epoch 00126: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9325 - dice_coef: 0.9325 - val_loss: -0.8294 - val_dice_coef: 0.8294\n",
            "Epoch 127/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9327 - dice_coef: 0.9327\n",
            "Epoch 00127: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.9327 - dice_coef: 0.9327 - val_loss: -0.8405 - val_dice_coef: 0.8405\n",
            "Epoch 128/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9351 - dice_coef: 0.9351\n",
            "Epoch 00128: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9351 - dice_coef: 0.9351 - val_loss: -0.8211 - val_dice_coef: 0.8211\n",
            "Epoch 129/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9328 - dice_coef: 0.9328\n",
            "Epoch 00129: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9328 - dice_coef: 0.9328 - val_loss: -0.8086 - val_dice_coef: 0.8086\n",
            "Epoch 130/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9363 - dice_coef: 0.9363\n",
            "Epoch 00130: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 602ms/step - loss: -0.9363 - dice_coef: 0.9363 - val_loss: -0.8260 - val_dice_coef: 0.8260\n",
            "Epoch 131/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9373 - dice_coef: 0.9373\n",
            "Epoch 00131: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9373 - dice_coef: 0.9373 - val_loss: -0.8304 - val_dice_coef: 0.8304\n",
            "Epoch 132/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9373 - dice_coef: 0.9373\n",
            "Epoch 00132: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9373 - dice_coef: 0.9373 - val_loss: -0.8077 - val_dice_coef: 0.8077\n",
            "Epoch 133/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9366 - dice_coef: 0.9366\n",
            "Epoch 00133: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9366 - dice_coef: 0.9366 - val_loss: -0.8212 - val_dice_coef: 0.8212\n",
            "Epoch 134/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9383 - dice_coef: 0.9383\n",
            "Epoch 00134: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9383 - dice_coef: 0.9383 - val_loss: -0.8180 - val_dice_coef: 0.8180\n",
            "Epoch 135/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9369 - dice_coef: 0.9369\n",
            "Epoch 00135: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.9369 - dice_coef: 0.9369 - val_loss: -0.8260 - val_dice_coef: 0.8260\n",
            "Epoch 136/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9383 - dice_coef: 0.9383\n",
            "Epoch 00136: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.9383 - dice_coef: 0.9383 - val_loss: -0.8241 - val_dice_coef: 0.8241\n",
            "Epoch 137/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9386 - dice_coef: 0.9386\n",
            "Epoch 00137: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 608ms/step - loss: -0.9386 - dice_coef: 0.9386 - val_loss: -0.8247 - val_dice_coef: 0.8247\n",
            "Epoch 138/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9380 - dice_coef: 0.9380\n",
            "Epoch 00138: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 607ms/step - loss: -0.9380 - dice_coef: 0.9380 - val_loss: -0.8332 - val_dice_coef: 0.8332\n",
            "Epoch 139/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9383 - dice_coef: 0.9383\n",
            "Epoch 00139: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9383 - dice_coef: 0.9383 - val_loss: -0.8256 - val_dice_coef: 0.8256\n",
            "Epoch 140/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9406 - dice_coef: 0.9406\n",
            "Epoch 00140: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9406 - dice_coef: 0.9406 - val_loss: -0.8206 - val_dice_coef: 0.8206\n",
            "Epoch 141/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9412 - dice_coef: 0.9412\n",
            "Epoch 00141: val_loss did not improve from -0.84463\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9412 - dice_coef: 0.9412 - val_loss: -0.8252 - val_dice_coef: 0.8252\n",
            "Epoch 142/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9402 - dice_coef: 0.9402\n",
            "Epoch 00142: val_loss improved from -0.84463 to -0.84470, saving model to saved_model/best_model.h5\n",
            "10/10 [==============================] - 6s 628ms/step - loss: -0.9402 - dice_coef: 0.9402 - val_loss: -0.8447 - val_dice_coef: 0.8447\n",
            "Epoch 143/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9398 - dice_coef: 0.9398\n",
            "Epoch 00143: val_loss did not improve from -0.84470\n",
            "10/10 [==============================] - 6s 604ms/step - loss: -0.9398 - dice_coef: 0.9398 - val_loss: -0.8364 - val_dice_coef: 0.8364\n",
            "Epoch 144/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9415 - dice_coef: 0.9415\n",
            "Epoch 00144: val_loss did not improve from -0.84470\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9415 - dice_coef: 0.9415 - val_loss: -0.8311 - val_dice_coef: 0.8311\n",
            "Epoch 145/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9395 - dice_coef: 0.9395\n",
            "Epoch 00145: val_loss did not improve from -0.84470\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9395 - dice_coef: 0.9395 - val_loss: -0.8434 - val_dice_coef: 0.8434\n",
            "Epoch 146/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9350 - dice_coef: 0.9350\n",
            "Epoch 00146: val_loss did not improve from -0.84470\n",
            "10/10 [==============================] - 6s 606ms/step - loss: -0.9350 - dice_coef: 0.9350 - val_loss: -0.8212 - val_dice_coef: 0.8212\n",
            "Epoch 147/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9388 - dice_coef: 0.9388\n",
            "Epoch 00147: val_loss did not improve from -0.84470\n",
            "10/10 [==============================] - 6s 603ms/step - loss: -0.9388 - dice_coef: 0.9388 - val_loss: -0.8294 - val_dice_coef: 0.8294\n",
            "Epoch 148/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9379 - dice_coef: 0.9379\n",
            "Epoch 00148: val_loss did not improve from -0.84470\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9379 - dice_coef: 0.9379 - val_loss: -0.8293 - val_dice_coef: 0.8293\n",
            "Epoch 149/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9399 - dice_coef: 0.9399\n",
            "Epoch 00149: val_loss did not improve from -0.84470\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9399 - dice_coef: 0.9399 - val_loss: -0.8100 - val_dice_coef: 0.8100\n",
            "Epoch 150/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9396 - dice_coef: 0.9396\n",
            "Epoch 00150: val_loss did not improve from -0.84470\n",
            "10/10 [==============================] - 6s 605ms/step - loss: -0.9396 - dice_coef: 0.9396 - val_loss: -0.8165 - val_dice_coef: 0.8165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bgiFiht6q_m",
        "colab_type": "text"
      },
      "source": [
        "Download Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNJvd4LgJV8s",
        "colab_type": "code",
        "outputId": "a40ecbba-2118-483c-c64c-33d6f17d427a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "!zip -r ./logs.zip ./logs/\n",
        "!zip -r ./saved_model ./saved_model/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: logs/ (stored 0%)\n",
            "  adding: logs/validation/ (stored 0%)\n",
            "  adding: logs/validation/events.out.tfevents.1588519179.8ce87b3a89ad.123.56650.v2 (deflated 67%)\n",
            "  adding: logs/validation/events.out.tfevents.1588518812.8ce87b3a89ad.123.34333.v2 (deflated 65%)\n",
            "  adding: logs/validation/events.out.tfevents.1588519723.8ce87b3a89ad.123.81013.v2 (deflated 68%)\n",
            "  adding: logs/validation/events.out.tfevents.1588518088.8ce87b3a89ad.123.7063.v2 (deflated 68%)\n",
            "  adding: logs/train/ (stored 0%)\n",
            "  adding: logs/train/events.out.tfevents.1588519130.8ce87b3a89ad.123.44933.v2 (deflated 93%)\n",
            "  adding: logs/train/events.out.tfevents.1588518083.8ce87b3a89ad.profile-empty (deflated 5%)\n",
            "  adding: logs/train/events.out.tfevents.1588518070.8ce87b3a89ad.123.1735.v2 (deflated 89%)\n",
            "  adding: logs/train/events.out.tfevents.1588518802.8ce87b3a89ad.123.28988.v2 (deflated 92%)\n",
            "  adding: logs/train/plugins/ (stored 0%)\n",
            "  adding: logs/train/plugins/profile/ (stored 0%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_01_23/ (stored 0%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_01_23/8ce87b3a89ad.trace.json.gz (deflated 1%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_01_23/8ce87b3a89ad.input_pipeline.pb (deflated 57%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_01_23/8ce87b3a89ad.kernel_stats.pb (deflated 96%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_01_23/8ce87b3a89ad.overview_page.pb (deflated 59%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_01_23/8ce87b3a89ad.tensorflow_stats.pb (deflated 77%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_13_25/ (stored 0%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_13_25/8ce87b3a89ad.trace.json.gz (deflated 1%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_13_25/8ce87b3a89ad.input_pipeline.pb (deflated 57%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_13_25/8ce87b3a89ad.kernel_stats.pb (deflated 96%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_13_25/8ce87b3a89ad.overview_page.pb (deflated 59%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_13_25/8ce87b3a89ad.tensorflow_stats.pb (deflated 78%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_18_53/ (stored 0%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_18_53/8ce87b3a89ad.trace.json.gz (deflated 1%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_18_53/8ce87b3a89ad.input_pipeline.pb (deflated 57%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_18_53/8ce87b3a89ad.kernel_stats.pb (deflated 96%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_18_53/8ce87b3a89ad.overview_page.pb (deflated 58%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_18_53/8ce87b3a89ad.tensorflow_stats.pb (deflated 79%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_19_33/ (stored 0%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_19_33/8ce87b3a89ad.trace.json.gz (deflated 1%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_19_33/8ce87b3a89ad.input_pipeline.pb (deflated 57%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_19_33/8ce87b3a89ad.kernel_stats.pb (deflated 96%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_19_33/8ce87b3a89ad.overview_page.pb (deflated 59%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_19_33/8ce87b3a89ad.tensorflow_stats.pb (deflated 78%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_28_37/ (stored 0%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_28_37/8ce87b3a89ad.trace.json.gz (deflated 1%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_28_37/8ce87b3a89ad.input_pipeline.pb (deflated 56%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_28_37/8ce87b3a89ad.kernel_stats.pb (deflated 96%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_28_37/8ce87b3a89ad.overview_page.pb (deflated 59%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_15_28_37/8ce87b3a89ad.tensorflow_stats.pb (deflated 78%)\n",
            "  adding: logs/train/events.out.tfevents.1588519169.8ce87b3a89ad.123.51305.v2 (deflated 93%)\n",
            "  adding: logs/train/events.out.tfevents.1588519713.8ce87b3a89ad.123.75668.v2 (deflated 92%)\n",
            "  adding: saved_model/ (stored 0%)\n",
            "  adding: saved_model/final_model.h5 (deflated 9%)\n",
            "  adding: saved_model/best_model.h5 (deflated 9%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FltekahIZHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}