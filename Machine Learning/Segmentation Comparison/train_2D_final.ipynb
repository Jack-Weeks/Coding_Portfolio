{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNet_2D_Cy(2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk-_EXONRs9K",
        "colab_type": "text"
      },
      "source": [
        "2D U-net Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9JSnssMRri3",
        "colab_type": "code",
        "outputId": "8c990343-21d8-4c9a-f762-c67640d52a05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import os\n",
        "from tensorflow.keras.utils import get_file\n",
        "import tensorflow as tf # version 2.2.0-rc2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import Adam, SGD\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "!pip install pyyaml h5py  # Required to save models in HDF5 format\n",
        "\n",
        "cwd = os.getcwd()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzcB3oocQsmD",
        "colab_type": "text"
      },
      "source": [
        "Load MRI Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuiuoHPoNGRI",
        "colab_type": "code",
        "outputId": "a8c28026-1438-4d92-bd7c-3011e134d032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Create folders and define paths\n",
        "\n",
        "!mkdir data\n",
        "!mkdir results\n",
        "!mkdir saved_model\n",
        "\n",
        "DATA_PATH = '/content/datasets-promise12'\n",
        "RESULT_PATH = '/content/results'\n",
        "\n",
        "print('Image and label data downloaded: ')\n",
        "print('Result directory created: <%s>.' % os.path.abspath(RESULT_PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image and label data downloaded: \n",
            "Result directory created: </content/results>.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px7xkGmeOOzO",
        "colab_type": "code",
        "outputId": "0b8e0fa2-2c97-48b7-bb8c-475af95d39ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import zipfile\n",
        "!wget https://github.com/gu98/MPHY0041_Segmentation/blob/master/data/datasets-promise12.zip?raw=true\n",
        "!unzip datasets-promise12.zip?raw=true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-03 16:56:20--  https://github.com/gu98/MPHY0041_Segmentation/blob/master/data/datasets-promise12.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.118.3\n",
            "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/gu98/MPHY0041_Segmentation/raw/master/data/datasets-promise12.zip [following]\n",
            "--2020-05-03 16:56:20--  https://github.com/gu98/MPHY0041_Segmentation/raw/master/data/datasets-promise12.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/gu98/MPHY0041_Segmentation/master/data/datasets-promise12.zip [following]\n",
            "--2020-05-03 16:56:20--  https://raw.githubusercontent.com/gu98/MPHY0041_Segmentation/master/data/datasets-promise12.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58033622 (55M) [application/zip]\n",
            "Saving to: ‘datasets-promise12.zip?raw=true’\n",
            "\n",
            "datasets-promise12. 100%[===================>]  55.34M   167MB/s    in 0.3s    \n",
            "\n",
            "2020-05-03 16:56:22 (167 MB/s) - ‘datasets-promise12.zip?raw=true’ saved [58033622/58033622]\n",
            "\n",
            "Archive:  datasets-promise12.zip?raw=true\n",
            "   creating: datasets-promise12/\n",
            "  inflating: datasets-promise12/label_train42.npy  \n",
            "  inflating: datasets-promise12/image_train27.npy  \n",
            "  inflating: datasets-promise12/image_train33.npy  \n",
            "  inflating: datasets-promise12/image_test10.npy  \n",
            "  inflating: datasets-promise12/image_test04.npy  \n",
            "  inflating: datasets-promise12/image_test05.npy  \n",
            "  inflating: datasets-promise12/image_test11.npy  \n",
            "  inflating: datasets-promise12/image_train32.npy  \n",
            "  inflating: datasets-promise12/image_train26.npy  \n",
            "  inflating: datasets-promise12/label_train43.npy  \n",
            "  inflating: datasets-promise12/label_train41.npy  \n",
            "  inflating: datasets-promise12/image_train18.npy  \n",
            "  inflating: datasets-promise12/image_train30.npy  \n",
            "  inflating: datasets-promise12/image_train24.npy  \n",
            "  inflating: datasets-promise12/image_test07.npy  \n",
            "  inflating: datasets-promise12/image_test13.npy  \n",
            "  inflating: datasets-promise12/image_test12.npy  \n",
            "  inflating: datasets-promise12/image_test06.npy  \n",
            "  inflating: datasets-promise12/image_train25.npy  \n",
            "  inflating: datasets-promise12/image_train31.npy  \n",
            "  inflating: datasets-promise12/image_train19.npy  \n",
            "  inflating: datasets-promise12/label_train40.npy  \n",
            "  inflating: datasets-promise12/label_train44.npy  \n",
            "  inflating: datasets-promise12/image_train35.npy  \n",
            "  inflating: datasets-promise12/image_train21.npy  \n",
            "  inflating: datasets-promise12/image_train09.npy  \n",
            "  inflating: datasets-promise12/image_test02.npy  \n",
            "  inflating: datasets-promise12/image_test16.npy  \n",
            "  inflating: datasets-promise12/image_test17.npy  \n",
            "  inflating: datasets-promise12/image_test03.npy  \n",
            "  inflating: datasets-promise12/image_train08.npy  \n",
            "  inflating: datasets-promise12/image_train20.npy  \n",
            "  inflating: datasets-promise12/image_train34.npy  \n",
            "  inflating: datasets-promise12/label_train45.npy  \n",
            "  inflating: datasets-promise12/label_train47.npy  \n",
            "  inflating: datasets-promise12/image_train22.npy  \n",
            "  inflating: datasets-promise12/image_train36.npy  \n",
            "  inflating: datasets-promise12/image_test29.npy  \n",
            "  inflating: datasets-promise12/image_test15.npy  \n",
            "  inflating: datasets-promise12/image_test01.npy  \n",
            "  inflating: datasets-promise12/image_test00.npy  \n",
            "  inflating: datasets-promise12/image_test14.npy  \n",
            "  inflating: datasets-promise12/image_test28.npy  \n",
            "  inflating: datasets-promise12/image_train37.npy  \n",
            "  inflating: datasets-promise12/image_train23.npy  \n",
            "  inflating: datasets-promise12/label_train46.npy  \n",
            "  inflating: datasets-promise12/label_train35.npy  \n",
            "  inflating: datasets-promise12/label_train21.npy  \n",
            "  inflating: datasets-promise12/label_train09.npy  \n",
            "  inflating: datasets-promise12/image_train44.npy  \n",
            "  inflating: datasets-promise12/image_train45.npy  \n",
            "  inflating: datasets-promise12/label_train08.npy  \n",
            "  inflating: datasets-promise12/label_train20.npy  \n",
            "  inflating: datasets-promise12/label_train34.npy  \n",
            "  inflating: datasets-promise12/label_train22.npy  \n",
            "  inflating: datasets-promise12/label_train36.npy  \n",
            "  inflating: datasets-promise12/image_train47.npy  \n",
            "  inflating: datasets-promise12/image_train46.npy  \n",
            "  inflating: datasets-promise12/label_train37.npy  \n",
            "  inflating: datasets-promise12/label_train23.npy  \n",
            "  inflating: datasets-promise12/label_train27.npy  \n",
            "  inflating: datasets-promise12/label_train33.npy  \n",
            "  inflating: datasets-promise12/image_train42.npy  \n",
            "  inflating: datasets-promise12/image_train43.npy  \n",
            "  inflating: datasets-promise12/label_train32.npy  \n",
            "  inflating: datasets-promise12/label_train26.npy  \n",
            "  inflating: datasets-promise12/label_train18.npy  \n",
            "  inflating: datasets-promise12/label_train30.npy  \n",
            "  inflating: datasets-promise12/label_train24.npy  \n",
            "  inflating: datasets-promise12/image_train41.npy  \n",
            "  inflating: datasets-promise12/image_train40.npy  \n",
            "  inflating: datasets-promise12/label_train25.npy  \n",
            "  inflating: datasets-promise12/label_train31.npy  \n",
            "  inflating: datasets-promise12/label_train19.npy  \n",
            "  inflating: datasets-promise12/label_train14.npy  \n",
            "  inflating: datasets-promise12/label_train00.npy  \n",
            "  inflating: datasets-promise12/label_train28.npy  \n",
            "  inflating: datasets-promise12/README.md  \n",
            "  inflating: datasets-promise12/label_train29.npy  \n",
            "  inflating: datasets-promise12/label_train01.npy  \n",
            "  inflating: datasets-promise12/label_train15.npy  \n",
            "  inflating: datasets-promise12/label_train03.npy  \n",
            "  inflating: datasets-promise12/label_train17.npy  \n",
            "  inflating: datasets-promise12/label_train16.npy  \n",
            "  inflating: datasets-promise12/label_train02.npy  \n",
            "  inflating: datasets-promise12/label_train06.npy  \n",
            "  inflating: datasets-promise12/label_train12.npy  \n",
            "  inflating: datasets-promise12/label_train13.npy  \n",
            "  inflating: datasets-promise12/label_train07.npy  \n",
            "  inflating: datasets-promise12/label_train39.npy  \n",
            "  inflating: datasets-promise12/label_train11.npy  \n",
            "  inflating: datasets-promise12/label_train05.npy  \n",
            "  inflating: datasets-promise12/image_train48.npy  \n",
            "  inflating: datasets-promise12/image_train49.npy  \n",
            "  inflating: datasets-promise12/label_train04.npy  \n",
            "  inflating: datasets-promise12/label_train10.npy  \n",
            "  inflating: datasets-promise12/label_train38.npy  \n",
            "  inflating: datasets-promise12/image_train06.npy  \n",
            "  inflating: datasets-promise12/image_train12.npy  \n",
            "  inflating: datasets-promise12/image_test25.npy  \n",
            "  inflating: datasets-promise12/image_test19.npy  \n",
            "  inflating: datasets-promise12/image_test18.npy  \n",
            "  inflating: datasets-promise12/image_test24.npy  \n",
            "  inflating: datasets-promise12/image_train13.npy  \n",
            "  inflating: datasets-promise12/image_train07.npy  \n",
            "  inflating: datasets-promise12/label_train48.npy  \n",
            "  inflating: datasets-promise12/image_train39.npy  \n",
            "  inflating: datasets-promise12/image_train11.npy  \n",
            "  inflating: datasets-promise12/image_train05.npy  \n",
            "  inflating: datasets-promise12/image_test26.npy  \n",
            "  inflating: datasets-promise12/image_test27.npy  \n",
            "  inflating: datasets-promise12/image_train04.npy  \n",
            "  inflating: datasets-promise12/image_train10.npy  \n",
            "  inflating: datasets-promise12/image_train38.npy  \n",
            "  inflating: datasets-promise12/label_train49.npy  \n",
            "  inflating: datasets-promise12/image_train14.npy  \n",
            "  inflating: datasets-promise12/image_train00.npy  \n",
            "  inflating: datasets-promise12/image_train28.npy  \n",
            "  inflating: datasets-promise12/image_test23.npy  \n",
            "  inflating: datasets-promise12/image_test22.npy  \n",
            "  inflating: datasets-promise12/image_train29.npy  \n",
            "  inflating: datasets-promise12/image_train01.npy  \n",
            "  inflating: datasets-promise12/image_train15.npy  \n",
            "  inflating: datasets-promise12/image_train03.npy  \n",
            "  inflating: datasets-promise12/image_train17.npy  \n",
            "  inflating: datasets-promise12/image_test08.npy  \n",
            "  inflating: datasets-promise12/image_test20.npy  \n",
            "  inflating: datasets-promise12/image_test21.npy  \n",
            "  inflating: datasets-promise12/image_test09.npy  \n",
            "  inflating: datasets-promise12/image_train16.npy  \n",
            "  inflating: datasets-promise12/image_train02.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ntiCWHbUNVl",
        "colab_type": "text"
      },
      "source": [
        "Set Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnJtvsUsOAQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "path_to_data = 'DATA_PATH'\n",
        "path_to_save = 'RESULT_PATH'\n",
        "\n",
        "N= 50\n",
        "N_test= 30\n",
        "\n",
        "img_width= 128\n",
        "img_height= 128\n",
        "img_thickness= 32\n",
        "img_channels= 1\n",
        "\n",
        "learning_rate= float(7.5e-4)\n",
        "epochs= 150\n",
        "val_size= float(1/9)\n",
        "test_size= 0.1\n",
        "dropout= 0.5\n",
        "batch_size= 128\n",
        "patience= 150\n",
        "\n",
        "#f= [0, 2, 4, 8, 16, 32, 16, 8, 4, 2]\n",
        "#f= [0, 4, 8, 16, 32, 64, 32, 16, 8, 4]\n",
        "#f= [0, 8, 16, 32, 64, 128, 64, 32, 16, 8]\n",
        "#f= [0, 16, 32, 64, 128, 256, 128, 64, 32, 16]\n",
        "num_channels = [0, 32, 64, 128, 256, 512, 256, 128, 64, 32]\n",
        "f = num_channels\n",
        "#f= [0, 64, 128, 256, 512, 1024, 512, 256, 128, 64]\n",
        "\n",
        "#f= [0, 128, 256, 512, 1024, 2048, 1024, 512, 256, 128]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dwrPF5JVHQ9",
        "colab_type": "text"
      },
      "source": [
        "Parse Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98LngX5YOGO7",
        "colab_type": "code",
        "outputId": "139ea700-9856-4aaf-b4de-e9ee4442dfe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Load Training Data\n",
        "X_train = np.zeros((N, img_thickness, img_height, img_width, img_channels), dtype=np.float32)\n",
        "Y_train = np.zeros((N, img_thickness, img_height, img_width, img_channels), dtype=np.float32)\n",
        "\n",
        "print('','','')\n",
        "print('','','')\n",
        "print('Loading Training Data')\n",
        "\n",
        "for n in tqdm(range(N)):\n",
        "    image = np.load(os.path.join(DATA_PATH, \"image_train%02d.npy\" % n))\n",
        "    label = np.load(os.path.join(DATA_PATH, \"label_train%02d.npy\" % n))\n",
        "    X_train[n] = image[:,:,:,np.newaxis]\n",
        "    Y_train[n] = label[:,:,:,np.newaxis]\n",
        "\n",
        "print('Loaded Training Data')\n",
        "X_train = np.reshape(X_train, (N*img_thickness, img_height, img_width, img_channels))\n",
        "Y_train = np.reshape(Y_train, (N*img_thickness, img_height, img_width, img_channels))\n",
        "\n",
        "tf.print(X_train.shape)\n",
        "\n",
        "X_train = X_train[:int(X_train.shape[0]*(1-test_size))]\n",
        "Y_train = Y_train[:int(Y_train.shape[0]*(1-test_size))]\n",
        "\n",
        "tf.print(X_train.shape)\n",
        "\n",
        "# Load Testing Data\n",
        "X_test = np.zeros((N_test, img_thickness, img_height, img_width, img_channels), dtype=np.float32)\n",
        "\n",
        "print('','','')\n",
        "print('','','')\n",
        "print('Loading Testing Data')\n",
        "\n",
        "for n in tqdm(range(N_test)):\n",
        "    image = np.load(os.path.join(DATA_PATH, \"image_test%02d.npy\" % n))\n",
        "    X_test[n] = image[:,:,:,np.newaxis]\n",
        "\n",
        "print('Loaded Testing Data')\n",
        "print('','','')\n",
        "print('','','')\n",
        "\n",
        "X_test = np.reshape(X_test, (N_test*img_thickness, img_height, img_width, img_channels))\n",
        "\n",
        "tf.print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 456.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  \n",
            "  \n",
            "Loading Training Data\n",
            "Loaded Training Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1600, 128, 128, 1)\n",
            "(1440, 128, 128, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [00:00<00:00, 891.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  \n",
            "  \n",
            "Loading Testing Data\n",
            "Loaded Testing Data\n",
            "  \n",
            "  \n",
            "(960, 128, 128, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj15fI63agNY",
        "colab_type": "text"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX7vZv9cahoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1.0):\n",
        "\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (\n",
        "        K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlPNlKqXaYXR",
        "colab_type": "text"
      },
      "source": [
        "U-Net Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26TFNZY0YeTL",
        "colab_type": "code",
        "outputId": "1c3c3bd8-bbe6-40ab-edad-a268cdea858e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# UNet Model\n",
        "inputs = tf.keras.layers.Input((img_width, img_height, img_channels))\n",
        "\n",
        "# Convert integers in image matrix to floating point \n",
        "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "\n",
        "# Encoding\n",
        "c1 = tf.keras.layers.Conv2D(f[1], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(s)\n",
        "c1 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c1)\n",
        "c1 = tf.keras.layers.Dropout(dropout)(c1)\n",
        "c2 = tf.keras.layers.Conv2D(f[1], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c1)\n",
        "p1 = tf.keras.layers.MaxPooling2D((2,2))(c1)\n",
        "\n",
        "\n",
        "c2 = tf.keras.layers.Conv2D(f[2], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(p1)\n",
        "c2 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c2)\n",
        "c2 = tf.keras.layers.Dropout(dropout)(c2)\n",
        "c2 = tf.keras.layers.Conv2D(f[2], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c2)\n",
        "p2 = tf.keras.layers.MaxPooling2D((2,2))(c2)\n",
        "\n",
        "\n",
        "c3 = tf.keras.layers.Conv2D(f[3], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(p2)\n",
        "c3 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c3)\n",
        "c3 = tf.keras.layers.Dropout(dropout)(c3)\n",
        "c3 = tf.keras.layers.Conv2D(f[3], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c3)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2,2))(c3)\n",
        "\n",
        "\n",
        "c4 = tf.keras.layers.Conv2D(f[4], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(p3)\n",
        "c4 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c4)\n",
        "c4 = tf.keras.layers.Dropout(dropout)(c4)\n",
        "c4 = tf.keras.layers.Conv2D(f[4], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c4)\n",
        "p4 = tf.keras.layers.MaxPooling2D((2,2))(c4)\n",
        "\n",
        "\n",
        "c5 = tf.keras.layers.Conv2D(f[5], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(p4)\n",
        "c5 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c5)\n",
        "c5 = tf.keras.layers.Dropout(dropout)(c5)\n",
        "c5 = tf.keras.layers.Conv2D(f[5], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c5)\n",
        "p5 = tf.keras.layers.MaxPooling2D((2,2))(c5)\n",
        "\n",
        "# Decoding Layers\n",
        "u6 = tf.keras.layers.Conv2DTranspose(f[6], (2, 2), strides=(2, 2), padding='same',)(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "c6 = tf.keras.layers.Conv2D(f[6], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(u6)\n",
        "c6 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c6)\n",
        "c6 = tf.keras.layers.Dropout(dropout)(c6)\n",
        "c6 = tf.keras.layers.Conv2D(f[6], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "\n",
        "u7 = tf.keras.layers.Conv2DTranspose(f[7], (2, 2), strides=(2, 2), padding='same',)(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "c7 = tf.keras.layers.Conv2D(f[7], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(u7)\n",
        "c7 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c7)\n",
        "c7 = tf.keras.layers.Dropout(dropout)(c7)\n",
        "c7 = tf.keras.layers.Conv2D(f[7], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "\n",
        "u8 = tf.keras.layers.Conv2DTranspose(f[8], (2, 2), strides=(2, 2), padding='same',)(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "c8 = tf.keras.layers.Conv2D(f[8], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(u8)\n",
        "c8 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c8)\n",
        "c8 = tf.keras.layers.Dropout(dropout)(c8)\n",
        "c8 = tf.keras.layers.Conv2D(f[8], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "\n",
        "u9 = tf.keras.layers.Conv2DTranspose(f[9], (2, 2), strides=(2, 2), padding='same',)(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, c1])\n",
        "c9 = tf.keras.layers.Conv2D(f[9], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(u9)\n",
        "c9 = tf.keras.layers.BatchNormalization(\n",
        "     axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(c9)\n",
        "c9 = tf.keras.layers.Dropout(dropout)(c9)\n",
        "c9 = tf.keras.layers.Conv2D(f[9], (3, 3), activation='relu',\n",
        "                            kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "\n",
        "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate), loss=dice_coef_loss, metrics=[dice_coef])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint('saved_model/2D_best_model.h5',\n",
        "                                                  verbose=1, save_best_only=True),\n",
        "            tf.keras.callbacks.EarlyStopping(patience=patience, monitor='loss'),\n",
        "            tf.keras.callbacks.TensorBoard(log_dir='logs')]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 128, 128, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 128, 128, 32) 320         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 128, 128, 32) 128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 128, 128, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 64, 64, 32)   0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 64, 64, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 64)   36928       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 128)  512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 128)  147584      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 128)  0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 256)  1024        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 16, 16, 256)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 256)  590080      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 8, 8, 512)    1180160     max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 8, 8, 512)    2048        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 8, 8, 512)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 8, 8, 512)    2359808     dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 16, 16, 256)  524544      conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 16, 16, 512)  0           conv2d_transpose[0][0]           \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 256)  1179904     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 16, 256)  1024        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 16, 16, 256)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 256)  590080      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 128)  131200      conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 256)  0           conv2d_transpose_1[0][0]         \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 128)  295040      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 128)  512         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 128)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 128)  147584      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 64)   32832       conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 64, 64, 128)  0           conv2d_transpose_2[0][0]         \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 64, 64, 64)   73792       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 64, 64, 64)   256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 64, 64, 64)   36928       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 32) 8224        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 128, 128, 64) 0           conv2d_transpose_3[0][0]         \n",
            "                                                                 dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 128, 128, 32) 18464       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 128, 128, 32) 128         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 128, 128, 32) 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 128, 128, 32) 9248        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 128, 128, 1)  33          conv2d_17[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 7,756,161\n",
            "Trainable params: 7,753,217\n",
            "Non-trainable params: 2,944\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy8_ZRB9baL1",
        "colab_type": "text"
      },
      "source": [
        "Train the 2D U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhN9Dv0faeSg",
        "colab_type": "code",
        "outputId": "17e4160f-ec0b-409e-b62b-a3e9ff4057eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results = model.fit(X_train, Y_train, validation_split=val_size, batch_size=batch_size,\n",
        "                    epochs=epochs, callbacks=callbacks) \n",
        "model.save('saved_model/2D_final_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.0771 - dice_coef: 0.0771\n",
            "Epoch 00001: val_loss improved from inf to -0.04275, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 5s 492ms/step - loss: -0.0771 - dice_coef: 0.0771 - val_loss: -0.0428 - val_dice_coef: 0.0428\n",
            "Epoch 2/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.1445 - dice_coef: 0.1445\n",
            "Epoch 00002: val_loss improved from -0.04275 to -0.04819, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 335ms/step - loss: -0.1445 - dice_coef: 0.1445 - val_loss: -0.0482 - val_dice_coef: 0.0482\n",
            "Epoch 3/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.2471 - dice_coef: 0.2471\n",
            "Epoch 00003: val_loss improved from -0.04819 to -0.16382, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 349ms/step - loss: -0.2471 - dice_coef: 0.2471 - val_loss: -0.1638 - val_dice_coef: 0.1638\n",
            "Epoch 4/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.4030 - dice_coef: 0.4030\n",
            "Epoch 00004: val_loss did not improve from -0.16382\n",
            "10/10 [==============================] - 3s 301ms/step - loss: -0.4030 - dice_coef: 0.4030 - val_loss: -0.0056 - val_dice_coef: 0.0056\n",
            "Epoch 5/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.5259 - dice_coef: 0.5259\n",
            "Epoch 00005: val_loss did not improve from -0.16382\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.5259 - dice_coef: 0.5259 - val_loss: -7.3751e-05 - val_dice_coef: 7.3751e-05\n",
            "Epoch 6/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.6044 - dice_coef: 0.6044\n",
            "Epoch 00006: val_loss did not improve from -0.16382\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.6044 - dice_coef: 0.6044 - val_loss: -7.0631e-05 - val_dice_coef: 7.0631e-05\n",
            "Epoch 7/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.6815 - dice_coef: 0.6815\n",
            "Epoch 00007: val_loss did not improve from -0.16382\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.6815 - dice_coef: 0.6815 - val_loss: -0.1245 - val_dice_coef: 0.1245\n",
            "Epoch 8/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7267 - dice_coef: 0.7267\n",
            "Epoch 00008: val_loss improved from -0.16382 to -0.46612, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 333ms/step - loss: -0.7267 - dice_coef: 0.7267 - val_loss: -0.4661 - val_dice_coef: 0.4661\n",
            "Epoch 9/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7534 - dice_coef: 0.7534\n",
            "Epoch 00009: val_loss improved from -0.46612 to -0.55629, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 345ms/step - loss: -0.7534 - dice_coef: 0.7534 - val_loss: -0.5563 - val_dice_coef: 0.5563\n",
            "Epoch 10/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7693 - dice_coef: 0.7693\n",
            "Epoch 00010: val_loss improved from -0.55629 to -0.57728, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 333ms/step - loss: -0.7693 - dice_coef: 0.7693 - val_loss: -0.5773 - val_dice_coef: 0.5773\n",
            "Epoch 11/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.7920 - dice_coef: 0.7920\n",
            "Epoch 00011: val_loss did not improve from -0.57728\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.7920 - dice_coef: 0.7920 - val_loss: -0.4356 - val_dice_coef: 0.4356\n",
            "Epoch 12/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8141 - dice_coef: 0.8141\n",
            "Epoch 00012: val_loss improved from -0.57728 to -0.69282, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 338ms/step - loss: -0.8141 - dice_coef: 0.8141 - val_loss: -0.6928 - val_dice_coef: 0.6928\n",
            "Epoch 13/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8134 - dice_coef: 0.8134\n",
            "Epoch 00013: val_loss did not improve from -0.69282\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.8134 - dice_coef: 0.8134 - val_loss: -0.6850 - val_dice_coef: 0.6850\n",
            "Epoch 14/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8324 - dice_coef: 0.8324\n",
            "Epoch 00014: val_loss did not improve from -0.69282\n",
            "10/10 [==============================] - 3s 302ms/step - loss: -0.8324 - dice_coef: 0.8324 - val_loss: -0.6791 - val_dice_coef: 0.6791\n",
            "Epoch 15/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8479 - dice_coef: 0.8479\n",
            "Epoch 00015: val_loss improved from -0.69282 to -0.72024, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 332ms/step - loss: -0.8479 - dice_coef: 0.8479 - val_loss: -0.7202 - val_dice_coef: 0.7202\n",
            "Epoch 16/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8450 - dice_coef: 0.8450\n",
            "Epoch 00016: val_loss did not improve from -0.72024\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.8450 - dice_coef: 0.8450 - val_loss: -0.7108 - val_dice_coef: 0.7108\n",
            "Epoch 17/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8535 - dice_coef: 0.8535\n",
            "Epoch 00017: val_loss improved from -0.72024 to -0.74412, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 334ms/step - loss: -0.8535 - dice_coef: 0.8535 - val_loss: -0.7441 - val_dice_coef: 0.7441\n",
            "Epoch 18/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8606 - dice_coef: 0.8606\n",
            "Epoch 00018: val_loss improved from -0.74412 to -0.74717, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 334ms/step - loss: -0.8606 - dice_coef: 0.8606 - val_loss: -0.7472 - val_dice_coef: 0.7472\n",
            "Epoch 19/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8627 - dice_coef: 0.8627\n",
            "Epoch 00019: val_loss did not improve from -0.74717\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.8627 - dice_coef: 0.8627 - val_loss: -0.7446 - val_dice_coef: 0.7446\n",
            "Epoch 20/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8723 - dice_coef: 0.8723\n",
            "Epoch 00020: val_loss improved from -0.74717 to -0.75847, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 336ms/step - loss: -0.8723 - dice_coef: 0.8723 - val_loss: -0.7585 - val_dice_coef: 0.7585\n",
            "Epoch 21/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8754 - dice_coef: 0.8754\n",
            "Epoch 00021: val_loss improved from -0.75847 to -0.77103, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 346ms/step - loss: -0.8754 - dice_coef: 0.8754 - val_loss: -0.7710 - val_dice_coef: 0.7710\n",
            "Epoch 22/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8742 - dice_coef: 0.8742\n",
            "Epoch 00022: val_loss did not improve from -0.77103\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.8742 - dice_coef: 0.8742 - val_loss: -0.7687 - val_dice_coef: 0.7687\n",
            "Epoch 23/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8805 - dice_coef: 0.8805\n",
            "Epoch 00023: val_loss did not improve from -0.77103\n",
            "10/10 [==============================] - 3s 301ms/step - loss: -0.8805 - dice_coef: 0.8805 - val_loss: -0.7299 - val_dice_coef: 0.7299\n",
            "Epoch 24/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8833 - dice_coef: 0.8833\n",
            "Epoch 00024: val_loss improved from -0.77103 to -0.77513, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 332ms/step - loss: -0.8833 - dice_coef: 0.8833 - val_loss: -0.7751 - val_dice_coef: 0.7751\n",
            "Epoch 25/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8836 - dice_coef: 0.8836\n",
            "Epoch 00025: val_loss improved from -0.77513 to -0.78026, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 342ms/step - loss: -0.8836 - dice_coef: 0.8836 - val_loss: -0.7803 - val_dice_coef: 0.7803\n",
            "Epoch 26/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8946 - dice_coef: 0.8946\n",
            "Epoch 00026: val_loss improved from -0.78026 to -0.78334, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 334ms/step - loss: -0.8946 - dice_coef: 0.8946 - val_loss: -0.7833 - val_dice_coef: 0.7833\n",
            "Epoch 27/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8943 - dice_coef: 0.8943\n",
            "Epoch 00027: val_loss improved from -0.78334 to -0.78930, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 339ms/step - loss: -0.8943 - dice_coef: 0.8943 - val_loss: -0.7893 - val_dice_coef: 0.7893\n",
            "Epoch 28/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8947 - dice_coef: 0.8947\n",
            "Epoch 00028: val_loss did not improve from -0.78930\n",
            "10/10 [==============================] - 3s 301ms/step - loss: -0.8947 - dice_coef: 0.8947 - val_loss: -0.7829 - val_dice_coef: 0.7829\n",
            "Epoch 29/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8959 - dice_coef: 0.8959\n",
            "Epoch 00029: val_loss did not improve from -0.78930\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.8959 - dice_coef: 0.8959 - val_loss: -0.7796 - val_dice_coef: 0.7796\n",
            "Epoch 30/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.8956 - dice_coef: 0.8956\n",
            "Epoch 00030: val_loss did not improve from -0.78930\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.8956 - dice_coef: 0.8956 - val_loss: -0.7841 - val_dice_coef: 0.7841\n",
            "Epoch 31/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9001 - dice_coef: 0.9001\n",
            "Epoch 00031: val_loss did not improve from -0.78930\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9001 - dice_coef: 0.9001 - val_loss: -0.7853 - val_dice_coef: 0.7853\n",
            "Epoch 32/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9006 - dice_coef: 0.9006\n",
            "Epoch 00032: val_loss improved from -0.78930 to -0.80422, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 331ms/step - loss: -0.9006 - dice_coef: 0.9006 - val_loss: -0.8042 - val_dice_coef: 0.8042\n",
            "Epoch 33/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9024 - dice_coef: 0.9024\n",
            "Epoch 00033: val_loss did not improve from -0.80422\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9024 - dice_coef: 0.9024 - val_loss: -0.7960 - val_dice_coef: 0.7960\n",
            "Epoch 34/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9047 - dice_coef: 0.9047\n",
            "Epoch 00034: val_loss did not improve from -0.80422\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9047 - dice_coef: 0.9047 - val_loss: -0.7850 - val_dice_coef: 0.7850\n",
            "Epoch 35/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9053 - dice_coef: 0.9053\n",
            "Epoch 00035: val_loss did not improve from -0.80422\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9053 - dice_coef: 0.9053 - val_loss: -0.7975 - val_dice_coef: 0.7975\n",
            "Epoch 36/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9036 - dice_coef: 0.9036\n",
            "Epoch 00036: val_loss did not improve from -0.80422\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9036 - dice_coef: 0.9036 - val_loss: -0.7958 - val_dice_coef: 0.7958\n",
            "Epoch 37/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9022 - dice_coef: 0.9022\n",
            "Epoch 00037: val_loss did not improve from -0.80422\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9022 - dice_coef: 0.9022 - val_loss: -0.7964 - val_dice_coef: 0.7964\n",
            "Epoch 38/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9061 - dice_coef: 0.9061\n",
            "Epoch 00038: val_loss did not improve from -0.80422\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9061 - dice_coef: 0.9061 - val_loss: -0.7745 - val_dice_coef: 0.7745\n",
            "Epoch 39/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9078 - dice_coef: 0.9078\n",
            "Epoch 00039: val_loss did not improve from -0.80422\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9078 - dice_coef: 0.9078 - val_loss: -0.7927 - val_dice_coef: 0.7927\n",
            "Epoch 40/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9129 - dice_coef: 0.9129\n",
            "Epoch 00040: val_loss did not improve from -0.80422\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9129 - dice_coef: 0.9129 - val_loss: -0.8032 - val_dice_coef: 0.8032\n",
            "Epoch 41/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9146 - dice_coef: 0.9146\n",
            "Epoch 00041: val_loss improved from -0.80422 to -0.82192, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 333ms/step - loss: -0.9146 - dice_coef: 0.9146 - val_loss: -0.8219 - val_dice_coef: 0.8219\n",
            "Epoch 42/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9149 - dice_coef: 0.9149\n",
            "Epoch 00042: val_loss did not improve from -0.82192\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9149 - dice_coef: 0.9149 - val_loss: -0.8128 - val_dice_coef: 0.8128\n",
            "Epoch 43/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9112 - dice_coef: 0.9112\n",
            "Epoch 00043: val_loss did not improve from -0.82192\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9112 - dice_coef: 0.9112 - val_loss: -0.8068 - val_dice_coef: 0.8068\n",
            "Epoch 44/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9137 - dice_coef: 0.9137\n",
            "Epoch 00044: val_loss did not improve from -0.82192\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9137 - dice_coef: 0.9137 - val_loss: -0.8160 - val_dice_coef: 0.8160\n",
            "Epoch 45/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9153 - dice_coef: 0.9153\n",
            "Epoch 00045: val_loss improved from -0.82192 to -0.82689, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 336ms/step - loss: -0.9153 - dice_coef: 0.9153 - val_loss: -0.8269 - val_dice_coef: 0.8269\n",
            "Epoch 46/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9192 - dice_coef: 0.9192\n",
            "Epoch 00046: val_loss did not improve from -0.82689\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9192 - dice_coef: 0.9192 - val_loss: -0.8147 - val_dice_coef: 0.8147\n",
            "Epoch 47/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9196 - dice_coef: 0.9196\n",
            "Epoch 00047: val_loss improved from -0.82689 to -0.83006, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 332ms/step - loss: -0.9196 - dice_coef: 0.9196 - val_loss: -0.8301 - val_dice_coef: 0.8301\n",
            "Epoch 48/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9146 - dice_coef: 0.9146\n",
            "Epoch 00048: val_loss did not improve from -0.83006\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9146 - dice_coef: 0.9146 - val_loss: -0.8253 - val_dice_coef: 0.8253\n",
            "Epoch 49/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9182 - dice_coef: 0.9182\n",
            "Epoch 00049: val_loss improved from -0.83006 to -0.83992, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 334ms/step - loss: -0.9182 - dice_coef: 0.9182 - val_loss: -0.8399 - val_dice_coef: 0.8399\n",
            "Epoch 50/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9143 - dice_coef: 0.9143\n",
            "Epoch 00050: val_loss did not improve from -0.83992\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9143 - dice_coef: 0.9143 - val_loss: -0.8282 - val_dice_coef: 0.8282\n",
            "Epoch 51/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9212 - dice_coef: 0.9212\n",
            "Epoch 00051: val_loss did not improve from -0.83992\n",
            "10/10 [==============================] - 3s 301ms/step - loss: -0.9212 - dice_coef: 0.9212 - val_loss: -0.8320 - val_dice_coef: 0.8320\n",
            "Epoch 52/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9235 - dice_coef: 0.9235\n",
            "Epoch 00052: val_loss improved from -0.83992 to -0.85578, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 332ms/step - loss: -0.9235 - dice_coef: 0.9235 - val_loss: -0.8558 - val_dice_coef: 0.8558\n",
            "Epoch 53/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9234 - dice_coef: 0.9234\n",
            "Epoch 00053: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 301ms/step - loss: -0.9234 - dice_coef: 0.9234 - val_loss: -0.8427 - val_dice_coef: 0.8427\n",
            "Epoch 54/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9198 - dice_coef: 0.9198\n",
            "Epoch 00054: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9198 - dice_coef: 0.9198 - val_loss: -0.8357 - val_dice_coef: 0.8357\n",
            "Epoch 55/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9206 - dice_coef: 0.9206\n",
            "Epoch 00055: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9206 - dice_coef: 0.9206 - val_loss: -0.8369 - val_dice_coef: 0.8369\n",
            "Epoch 56/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9225 - dice_coef: 0.9225\n",
            "Epoch 00056: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 297ms/step - loss: -0.9225 - dice_coef: 0.9225 - val_loss: -0.8450 - val_dice_coef: 0.8450\n",
            "Epoch 57/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9226 - dice_coef: 0.9226\n",
            "Epoch 00057: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9226 - dice_coef: 0.9226 - val_loss: -0.8444 - val_dice_coef: 0.8444\n",
            "Epoch 58/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9265 - dice_coef: 0.9265\n",
            "Epoch 00058: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9265 - dice_coef: 0.9265 - val_loss: -0.8442 - val_dice_coef: 0.8442\n",
            "Epoch 59/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9245 - dice_coef: 0.9245\n",
            "Epoch 00059: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9245 - dice_coef: 0.9245 - val_loss: -0.8350 - val_dice_coef: 0.8350\n",
            "Epoch 60/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9241 - dice_coef: 0.9241\n",
            "Epoch 00060: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9241 - dice_coef: 0.9241 - val_loss: -0.8386 - val_dice_coef: 0.8386\n",
            "Epoch 61/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9286 - dice_coef: 0.9286\n",
            "Epoch 00061: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9286 - dice_coef: 0.9286 - val_loss: -0.8175 - val_dice_coef: 0.8175\n",
            "Epoch 62/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9257 - dice_coef: 0.9257\n",
            "Epoch 00062: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9257 - dice_coef: 0.9257 - val_loss: -0.8359 - val_dice_coef: 0.8359\n",
            "Epoch 63/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9252 - dice_coef: 0.9252\n",
            "Epoch 00063: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9252 - dice_coef: 0.9252 - val_loss: -0.8253 - val_dice_coef: 0.8253\n",
            "Epoch 64/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9272 - dice_coef: 0.9272\n",
            "Epoch 00064: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9272 - dice_coef: 0.9272 - val_loss: -0.8207 - val_dice_coef: 0.8207\n",
            "Epoch 65/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9269 - dice_coef: 0.9269\n",
            "Epoch 00065: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9269 - dice_coef: 0.9269 - val_loss: -0.8338 - val_dice_coef: 0.8338\n",
            "Epoch 66/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9300 - dice_coef: 0.9300\n",
            "Epoch 00066: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9300 - dice_coef: 0.9300 - val_loss: -0.8265 - val_dice_coef: 0.8265\n",
            "Epoch 67/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9302 - dice_coef: 0.9302\n",
            "Epoch 00067: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9302 - dice_coef: 0.9302 - val_loss: -0.8234 - val_dice_coef: 0.8234\n",
            "Epoch 68/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9299 - dice_coef: 0.9299\n",
            "Epoch 00068: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9299 - dice_coef: 0.9299 - val_loss: -0.8184 - val_dice_coef: 0.8184\n",
            "Epoch 69/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9332 - dice_coef: 0.9332\n",
            "Epoch 00069: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9332 - dice_coef: 0.9332 - val_loss: -0.8346 - val_dice_coef: 0.8346\n",
            "Epoch 70/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9323 - dice_coef: 0.9323\n",
            "Epoch 00070: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9323 - dice_coef: 0.9323 - val_loss: -0.8322 - val_dice_coef: 0.8322\n",
            "Epoch 71/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9335 - dice_coef: 0.9335\n",
            "Epoch 00071: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9335 - dice_coef: 0.9335 - val_loss: -0.8425 - val_dice_coef: 0.8425\n",
            "Epoch 72/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9352 - dice_coef: 0.9352\n",
            "Epoch 00072: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9352 - dice_coef: 0.9352 - val_loss: -0.8425 - val_dice_coef: 0.8425\n",
            "Epoch 73/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9360 - dice_coef: 0.9360\n",
            "Epoch 00073: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9360 - dice_coef: 0.9360 - val_loss: -0.8509 - val_dice_coef: 0.8509\n",
            "Epoch 74/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9376 - dice_coef: 0.9376\n",
            "Epoch 00074: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9376 - dice_coef: 0.9376 - val_loss: -0.8469 - val_dice_coef: 0.8469\n",
            "Epoch 75/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9384 - dice_coef: 0.9384\n",
            "Epoch 00075: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9384 - dice_coef: 0.9384 - val_loss: -0.8351 - val_dice_coef: 0.8351\n",
            "Epoch 76/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9366 - dice_coef: 0.9366\n",
            "Epoch 00076: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9366 - dice_coef: 0.9366 - val_loss: -0.8398 - val_dice_coef: 0.8398\n",
            "Epoch 77/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9362 - dice_coef: 0.9362\n",
            "Epoch 00077: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9362 - dice_coef: 0.9362 - val_loss: -0.8409 - val_dice_coef: 0.8409\n",
            "Epoch 78/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9348 - dice_coef: 0.9348\n",
            "Epoch 00078: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 301ms/step - loss: -0.9348 - dice_coef: 0.9348 - val_loss: -0.8383 - val_dice_coef: 0.8383\n",
            "Epoch 79/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9353 - dice_coef: 0.9353\n",
            "Epoch 00079: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9353 - dice_coef: 0.9353 - val_loss: -0.8375 - val_dice_coef: 0.8375\n",
            "Epoch 80/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9364 - dice_coef: 0.9364\n",
            "Epoch 00080: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9364 - dice_coef: 0.9364 - val_loss: -0.8312 - val_dice_coef: 0.8312\n",
            "Epoch 81/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9364 - dice_coef: 0.9364\n",
            "Epoch 00081: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9364 - dice_coef: 0.9364 - val_loss: -0.8460 - val_dice_coef: 0.8460\n",
            "Epoch 82/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9364 - dice_coef: 0.9364\n",
            "Epoch 00082: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9364 - dice_coef: 0.9364 - val_loss: -0.8501 - val_dice_coef: 0.8501\n",
            "Epoch 83/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9383 - dice_coef: 0.9383\n",
            "Epoch 00083: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9383 - dice_coef: 0.9383 - val_loss: -0.8499 - val_dice_coef: 0.8499\n",
            "Epoch 84/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9368 - dice_coef: 0.9368\n",
            "Epoch 00084: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9368 - dice_coef: 0.9368 - val_loss: -0.8348 - val_dice_coef: 0.8348\n",
            "Epoch 85/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9378 - dice_coef: 0.9378\n",
            "Epoch 00085: val_loss did not improve from -0.85578\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9378 - dice_coef: 0.9378 - val_loss: -0.8385 - val_dice_coef: 0.8385\n",
            "Epoch 86/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9362 - dice_coef: 0.9362\n",
            "Epoch 00086: val_loss improved from -0.85578 to -0.85619, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 334ms/step - loss: -0.9362 - dice_coef: 0.9362 - val_loss: -0.8562 - val_dice_coef: 0.8562\n",
            "Epoch 87/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9380 - dice_coef: 0.9380\n",
            "Epoch 00087: val_loss did not improve from -0.85619\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9380 - dice_coef: 0.9380 - val_loss: -0.8357 - val_dice_coef: 0.8357\n",
            "Epoch 88/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9387 - dice_coef: 0.9387\n",
            "Epoch 00088: val_loss did not improve from -0.85619\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9387 - dice_coef: 0.9387 - val_loss: -0.8335 - val_dice_coef: 0.8335\n",
            "Epoch 89/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9396 - dice_coef: 0.9396\n",
            "Epoch 00089: val_loss did not improve from -0.85619\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9396 - dice_coef: 0.9396 - val_loss: -0.8487 - val_dice_coef: 0.8487\n",
            "Epoch 90/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9401 - dice_coef: 0.9401\n",
            "Epoch 00090: val_loss did not improve from -0.85619\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9401 - dice_coef: 0.9401 - val_loss: -0.8392 - val_dice_coef: 0.8392\n",
            "Epoch 91/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9402 - dice_coef: 0.9402\n",
            "Epoch 00091: val_loss did not improve from -0.85619\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9402 - dice_coef: 0.9402 - val_loss: -0.8346 - val_dice_coef: 0.8346\n",
            "Epoch 92/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9387 - dice_coef: 0.9387\n",
            "Epoch 00092: val_loss did not improve from -0.85619\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9387 - dice_coef: 0.9387 - val_loss: -0.8407 - val_dice_coef: 0.8407\n",
            "Epoch 93/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9404 - dice_coef: 0.9404\n",
            "Epoch 00093: val_loss did not improve from -0.85619\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9404 - dice_coef: 0.9404 - val_loss: -0.8488 - val_dice_coef: 0.8488\n",
            "Epoch 94/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9424 - dice_coef: 0.9424\n",
            "Epoch 00094: val_loss did not improve from -0.85619\n",
            "10/10 [==============================] - 3s 297ms/step - loss: -0.9424 - dice_coef: 0.9424 - val_loss: -0.8500 - val_dice_coef: 0.8500\n",
            "Epoch 95/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9439 - dice_coef: 0.9439\n",
            "Epoch 00095: val_loss improved from -0.85619 to -0.85956, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 334ms/step - loss: -0.9439 - dice_coef: 0.9439 - val_loss: -0.8596 - val_dice_coef: 0.8596\n",
            "Epoch 96/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9395 - dice_coef: 0.9395\n",
            "Epoch 00096: val_loss did not improve from -0.85956\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9395 - dice_coef: 0.9395 - val_loss: -0.8428 - val_dice_coef: 0.8428\n",
            "Epoch 97/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9418 - dice_coef: 0.9418\n",
            "Epoch 00097: val_loss improved from -0.85956 to -0.86075, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 335ms/step - loss: -0.9418 - dice_coef: 0.9418 - val_loss: -0.8608 - val_dice_coef: 0.8608\n",
            "Epoch 98/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9380 - dice_coef: 0.9380\n",
            "Epoch 00098: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9380 - dice_coef: 0.9380 - val_loss: -0.8524 - val_dice_coef: 0.8524\n",
            "Epoch 99/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9369 - dice_coef: 0.9369\n",
            "Epoch 00099: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9369 - dice_coef: 0.9369 - val_loss: -0.8327 - val_dice_coef: 0.8327\n",
            "Epoch 100/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9389 - dice_coef: 0.9389\n",
            "Epoch 00100: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9389 - dice_coef: 0.9389 - val_loss: -0.8361 - val_dice_coef: 0.8361\n",
            "Epoch 101/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9390 - dice_coef: 0.9390\n",
            "Epoch 00101: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 297ms/step - loss: -0.9390 - dice_coef: 0.9390 - val_loss: -0.8559 - val_dice_coef: 0.8559\n",
            "Epoch 102/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9410 - dice_coef: 0.9410\n",
            "Epoch 00102: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9410 - dice_coef: 0.9410 - val_loss: -0.8226 - val_dice_coef: 0.8226\n",
            "Epoch 103/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9418 - dice_coef: 0.9418\n",
            "Epoch 00103: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9418 - dice_coef: 0.9418 - val_loss: -0.8333 - val_dice_coef: 0.8333\n",
            "Epoch 104/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9450 - dice_coef: 0.9450\n",
            "Epoch 00104: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9450 - dice_coef: 0.9450 - val_loss: -0.8454 - val_dice_coef: 0.8454\n",
            "Epoch 105/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9458 - dice_coef: 0.9458\n",
            "Epoch 00105: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9458 - dice_coef: 0.9458 - val_loss: -0.8382 - val_dice_coef: 0.8382\n",
            "Epoch 106/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9453 - dice_coef: 0.9453\n",
            "Epoch 00106: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 297ms/step - loss: -0.9453 - dice_coef: 0.9453 - val_loss: -0.8304 - val_dice_coef: 0.8304\n",
            "Epoch 107/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9447 - dice_coef: 0.9447\n",
            "Epoch 00107: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9447 - dice_coef: 0.9447 - val_loss: -0.8249 - val_dice_coef: 0.8249\n",
            "Epoch 108/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9457 - dice_coef: 0.9457\n",
            "Epoch 00108: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9457 - dice_coef: 0.9457 - val_loss: -0.8448 - val_dice_coef: 0.8448\n",
            "Epoch 109/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9451 - dice_coef: 0.9451\n",
            "Epoch 00109: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9451 - dice_coef: 0.9451 - val_loss: -0.8492 - val_dice_coef: 0.8492\n",
            "Epoch 110/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9446 - dice_coef: 0.9446\n",
            "Epoch 00110: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9446 - dice_coef: 0.9446 - val_loss: -0.8435 - val_dice_coef: 0.8435\n",
            "Epoch 111/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9444 - dice_coef: 0.9444\n",
            "Epoch 00111: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9444 - dice_coef: 0.9444 - val_loss: -0.8516 - val_dice_coef: 0.8516\n",
            "Epoch 112/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9453 - dice_coef: 0.9453\n",
            "Epoch 00112: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9453 - dice_coef: 0.9453 - val_loss: -0.8449 - val_dice_coef: 0.8449\n",
            "Epoch 113/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9443 - dice_coef: 0.9443\n",
            "Epoch 00113: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9443 - dice_coef: 0.9443 - val_loss: -0.8419 - val_dice_coef: 0.8419\n",
            "Epoch 114/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9456 - dice_coef: 0.9456\n",
            "Epoch 00114: val_loss did not improve from -0.86075\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9456 - dice_coef: 0.9456 - val_loss: -0.8551 - val_dice_coef: 0.8551\n",
            "Epoch 115/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9454 - dice_coef: 0.9454\n",
            "Epoch 00115: val_loss improved from -0.86075 to -0.86456, saving model to saved_model/2D_best_model.h5\n",
            "10/10 [==============================] - 3s 334ms/step - loss: -0.9454 - dice_coef: 0.9454 - val_loss: -0.8646 - val_dice_coef: 0.8646\n",
            "Epoch 116/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9476 - dice_coef: 0.9476\n",
            "Epoch 00116: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9476 - dice_coef: 0.9476 - val_loss: -0.8492 - val_dice_coef: 0.8492\n",
            "Epoch 117/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9488 - dice_coef: 0.9488\n",
            "Epoch 00117: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9488 - dice_coef: 0.9488 - val_loss: -0.8498 - val_dice_coef: 0.8498\n",
            "Epoch 118/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9454 - dice_coef: 0.9454\n",
            "Epoch 00118: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9454 - dice_coef: 0.9454 - val_loss: -0.8635 - val_dice_coef: 0.8635\n",
            "Epoch 119/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9439 - dice_coef: 0.9439\n",
            "Epoch 00119: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9439 - dice_coef: 0.9439 - val_loss: -0.8577 - val_dice_coef: 0.8577\n",
            "Epoch 120/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9468 - dice_coef: 0.9468\n",
            "Epoch 00120: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9468 - dice_coef: 0.9468 - val_loss: -0.8471 - val_dice_coef: 0.8471\n",
            "Epoch 121/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9456 - dice_coef: 0.9456\n",
            "Epoch 00121: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9456 - dice_coef: 0.9456 - val_loss: -0.8321 - val_dice_coef: 0.8321\n",
            "Epoch 122/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9480 - dice_coef: 0.9480\n",
            "Epoch 00122: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9480 - dice_coef: 0.9480 - val_loss: -0.8207 - val_dice_coef: 0.8207\n",
            "Epoch 123/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9495 - dice_coef: 0.9495\n",
            "Epoch 00123: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9495 - dice_coef: 0.9495 - val_loss: -0.8421 - val_dice_coef: 0.8421\n",
            "Epoch 124/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9487 - dice_coef: 0.9487\n",
            "Epoch 00124: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9487 - dice_coef: 0.9487 - val_loss: -0.8390 - val_dice_coef: 0.8390\n",
            "Epoch 125/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9497 - dice_coef: 0.9497\n",
            "Epoch 00125: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 297ms/step - loss: -0.9497 - dice_coef: 0.9497 - val_loss: -0.8343 - val_dice_coef: 0.8343\n",
            "Epoch 126/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9496 - dice_coef: 0.9496\n",
            "Epoch 00126: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9496 - dice_coef: 0.9496 - val_loss: -0.8386 - val_dice_coef: 0.8386\n",
            "Epoch 127/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9504 - dice_coef: 0.9504\n",
            "Epoch 00127: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9504 - dice_coef: 0.9504 - val_loss: -0.8313 - val_dice_coef: 0.8313\n",
            "Epoch 128/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9466 - dice_coef: 0.9466\n",
            "Epoch 00128: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9466 - dice_coef: 0.9466 - val_loss: -0.8426 - val_dice_coef: 0.8426\n",
            "Epoch 129/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9475 - dice_coef: 0.9475\n",
            "Epoch 00129: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9475 - dice_coef: 0.9475 - val_loss: -0.8458 - val_dice_coef: 0.8458\n",
            "Epoch 130/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9491 - dice_coef: 0.9491\n",
            "Epoch 00130: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9491 - dice_coef: 0.9491 - val_loss: -0.8515 - val_dice_coef: 0.8515\n",
            "Epoch 131/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9506 - dice_coef: 0.9506\n",
            "Epoch 00131: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9506 - dice_coef: 0.9506 - val_loss: -0.8466 - val_dice_coef: 0.8466\n",
            "Epoch 132/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9520 - dice_coef: 0.9520\n",
            "Epoch 00132: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 300ms/step - loss: -0.9520 - dice_coef: 0.9520 - val_loss: -0.8460 - val_dice_coef: 0.8460\n",
            "Epoch 133/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9513 - dice_coef: 0.9513\n",
            "Epoch 00133: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9513 - dice_coef: 0.9513 - val_loss: -0.8584 - val_dice_coef: 0.8584\n",
            "Epoch 134/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9520 - dice_coef: 0.9520\n",
            "Epoch 00134: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9520 - dice_coef: 0.9520 - val_loss: -0.8495 - val_dice_coef: 0.8495\n",
            "Epoch 135/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9511 - dice_coef: 0.9511\n",
            "Epoch 00135: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 301ms/step - loss: -0.9511 - dice_coef: 0.9511 - val_loss: -0.8527 - val_dice_coef: 0.8527\n",
            "Epoch 136/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9512 - dice_coef: 0.9512\n",
            "Epoch 00136: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9512 - dice_coef: 0.9512 - val_loss: -0.8565 - val_dice_coef: 0.8565\n",
            "Epoch 137/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9518 - dice_coef: 0.9518\n",
            "Epoch 00137: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9518 - dice_coef: 0.9518 - val_loss: -0.8424 - val_dice_coef: 0.8424\n",
            "Epoch 138/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9509 - dice_coef: 0.9509\n",
            "Epoch 00138: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9509 - dice_coef: 0.9509 - val_loss: -0.8577 - val_dice_coef: 0.8577\n",
            "Epoch 139/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9509 - dice_coef: 0.9509\n",
            "Epoch 00139: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9509 - dice_coef: 0.9509 - val_loss: -0.8500 - val_dice_coef: 0.8500\n",
            "Epoch 140/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9500 - dice_coef: 0.9500\n",
            "Epoch 00140: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9500 - dice_coef: 0.9500 - val_loss: -0.8636 - val_dice_coef: 0.8636\n",
            "Epoch 141/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9528 - dice_coef: 0.9528\n",
            "Epoch 00141: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 301ms/step - loss: -0.9528 - dice_coef: 0.9528 - val_loss: -0.8477 - val_dice_coef: 0.8477\n",
            "Epoch 142/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9515 - dice_coef: 0.9515\n",
            "Epoch 00142: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9515 - dice_coef: 0.9515 - val_loss: -0.8206 - val_dice_coef: 0.8206\n",
            "Epoch 143/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9520 - dice_coef: 0.9520\n",
            "Epoch 00143: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9520 - dice_coef: 0.9520 - val_loss: -0.8412 - val_dice_coef: 0.8412\n",
            "Epoch 144/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9513 - dice_coef: 0.9513\n",
            "Epoch 00144: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9513 - dice_coef: 0.9513 - val_loss: -0.8405 - val_dice_coef: 0.8405\n",
            "Epoch 145/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9521 - dice_coef: 0.9521\n",
            "Epoch 00145: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9521 - dice_coef: 0.9521 - val_loss: -0.8430 - val_dice_coef: 0.8430\n",
            "Epoch 146/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9525 - dice_coef: 0.9525\n",
            "Epoch 00146: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9525 - dice_coef: 0.9525 - val_loss: -0.8502 - val_dice_coef: 0.8502\n",
            "Epoch 147/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9518 - dice_coef: 0.9518\n",
            "Epoch 00147: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9518 - dice_coef: 0.9518 - val_loss: -0.8643 - val_dice_coef: 0.8643\n",
            "Epoch 148/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9550 - dice_coef: 0.9550\n",
            "Epoch 00148: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9550 - dice_coef: 0.9550 - val_loss: -0.8612 - val_dice_coef: 0.8612\n",
            "Epoch 149/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9506 - dice_coef: 0.9506\n",
            "Epoch 00149: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 299ms/step - loss: -0.9506 - dice_coef: 0.9506 - val_loss: -0.8597 - val_dice_coef: 0.8597\n",
            "Epoch 150/150\n",
            "10/10 [==============================] - ETA: 0s - loss: -0.9516 - dice_coef: 0.9516\n",
            "Epoch 00150: val_loss did not improve from -0.86456\n",
            "10/10 [==============================] - 3s 298ms/step - loss: -0.9516 - dice_coef: 0.9516 - val_loss: -0.8477 - val_dice_coef: 0.8477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bgiFiht6q_m",
        "colab_type": "text"
      },
      "source": [
        "Download Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNJvd4LgJV8s",
        "colab_type": "code",
        "outputId": "2f6f91a0-64e5-4563-f5c1-25511ccbddbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!zip -r ./logs.zip ./logs/\n",
        "!zip -r ./saved_model ./saved_model/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: logs/ (stored 0%)\n",
            "  adding: logs/validation/ (stored 0%)\n",
            "  adding: logs/validation/events.out.tfevents.1588525045.29c92e7ba36b.121.6203.v2 (deflated 68%)\n",
            "  adding: logs/train/ (stored 0%)\n",
            "  adding: logs/train/events.out.tfevents.1588525041.29c92e7ba36b.profile-empty (deflated 5%)\n",
            "  adding: logs/train/plugins/ (stored 0%)\n",
            "  adding: logs/train/plugins/profile/ (stored 0%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_16_57_21/ (stored 0%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_16_57_21/29c92e7ba36b.overview_page.pb (deflated 59%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_16_57_21/29c92e7ba36b.trace.json.gz (deflated 0%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_16_57_21/29c92e7ba36b.input_pipeline.pb (deflated 57%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_16_57_21/29c92e7ba36b.kernel_stats.pb (deflated 94%)\n",
            "  adding: logs/train/plugins/profile/2020_05_03_16_57_21/29c92e7ba36b.tensorflow_stats.pb (deflated 71%)\n",
            "  adding: logs/train/events.out.tfevents.1588525029.29c92e7ba36b.121.1834.v2 (deflated 90%)\n",
            "  adding: saved_model/ (stored 0%)\n",
            "  adding: saved_model/2D_final_model.h5 (deflated 8%)\n",
            "  adding: saved_model/2D_best_model.h5 (deflated 8%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FltekahIZHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}